{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# CaptchaTrain.py\n",
    "from CaptchaGenerator import char_set\n",
    "from CaptchaGenerator import gen_captcha_text_and_image, convert_image_to_gray, generate_captcha_text_label, \\\n",
    "   text_label_turn_to_char_list, pred_label_turn_to_char_list\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# 基本参数设置：验证码图像的大小、验证码字符串长度、训练批样本大小\n",
    "IMAGE_WIDTH = 160\n",
    "IMAGE_HEIGHT = 60\n",
    "TEXT_LEN = 4\n",
    "CHAR_SET = char_set\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SIZE = 100\n",
    "TEST_SIZE = 100\n",
    "ITERATION = 5\n",
    "\n",
    "if not os.path.exists(\"./tmp/\"):\n",
    "   os.mkdir(\"./tmp\")\n",
    "if not os.path.exists(\"./train_image/\"):\n",
    "   os.mkdir(\"./train_image/\")\n",
    "if not os.path.exists(\"./validation_image/\"):\n",
    "   os.mkdir(\"./validation_image/\")\n",
    "if not os.path.exists(\"./test_image/\"):\n",
    "   os.mkdir(\"./test_image/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一个训练batch\n",
    "def get_next_batch(batch_size, mode=\"train\"):\n",
    "   im_save_path = \"\"\n",
    "   batch_x = np.zeros(shape=[batch_size, IMAGE_WIDTH * IMAGE_HEIGHT])\n",
    "   batch_y = np.zeros(shape=[batch_size, TEXT_LEN * len(CHAR_SET)])\n",
    "   for index in range(batch_size):\n",
    "      tex, img = gen_captcha_text_and_image(IMAGE_WIDTH, IMAGE_HEIGHT, TEXT_LEN, CHAR_SET)\n",
    "      im = Image.fromarray(img)\n",
    "      if mode == \"train\":# 训练集\n",
    "         im_save_path = \"./train_image/\" + str(index) + \"_train.jpg\"\n",
    "      elif mode == \"validation\":# 验证集\n",
    "         im_save_path = \"./validation_image/\" + str(index) + \"_validation.jpg\"\n",
    "      elif mode == \"test\":# 测试集\n",
    "         im_save_path = \"./test_image/\" + str(index) + \"_test.jpg\"\n",
    "      im.save(im_save_path)\n",
    "      gray_image, gray_image_matrix = convert_image_to_gray(img)\n",
    "      batch_x[index, :] = gray_image_matrix\n",
    "      batch_y[index, :] = generate_captcha_text_label(tex, CHAR_SET)\n",
    "\n",
    "   return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "# batch_x, batch_y = get_next_batch(BATCH_SIZE)\n",
    "# print(batch_x[0], batch_y[0])\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, IMAGE_HEIGHT * IMAGE_WIDTH])\n",
    "Y = tf.placeholder(tf.float32, [None, TEXT_LEN * len(CHAR_SET)])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# 定义生成w变量的函数\n",
    "def weight_variable(shape):\n",
    "   initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "   return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# 定义生成b变量的函数\n",
    "def bias_variable(shape):\n",
    "   initial = tf.constant(0.1, shape=shape)\n",
    "   return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# 定义卷积函数,x是输入的图像，W是此卷积层的权重矩阵\n",
    "def conv2d(x, w):\n",
    "   return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "# 定义池化函数，x是输入的矩阵\n",
    "def max_pool2x2(x):\n",
    "   return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络函数\n",
    "def cnn(x_input, hei, wid, text_len, len_char_set):\n",
    "   x = tf.reshape(x_input, shape=[-1, hei, wid, 1])\n",
    "   # x：一个shape为(BATCH_SIZE,160,60,1)的张量\n",
    "   # conv_1p输出：一个shape为(BATCH_SIZE,80,30,32)的张量\n",
    "   # conv_2p输出：一个shape为(BATCH_SIZE,40,15,64)的张量\n",
    "   # conv_3p输出：一个shape为(BATCH_SIZE,20,8,64)的张量\n",
    "   # conv_3r输出：一个shape为(BATCH_SIZE,20X8X64)的张量\n",
    "   # fc_1输出：一个shape为(BATCH_SIZE,1024)的张量\n",
    "   # out输出：一个shape为(BATCH_SIZE,40)的张量\n",
    "   conv_width_input = wid\n",
    "   conv_height_input = hei\n",
    "   w_c1 = weight_variable([3, 3, 1, 32])\n",
    "   b_c1 = bias_variable([32])\n",
    "   w_c2 = weight_variable([3, 3, 32, 64])\n",
    "   b_c2 = bias_variable([64])\n",
    "   w_c3 = weight_variable([3, 3, 64, 64])\n",
    "   b_c3 = bias_variable([64])\n",
    "   full_connect_width = math.ceil(conv_width_input / 8)\n",
    "   full_connect_height = math.ceil(conv_height_input / 8)\n",
    "   w_d1 = weight_variable([full_connect_width * full_connect_height * 64, 1024])\n",
    "   b_d1 = bias_variable([1024])\n",
    "   w_out = weight_variable([1024, text_len * len_char_set])\n",
    "   b_out = bias_variable([text_len * len_char_set])\n",
    "\n",
    "   conv_1c = tf.nn.relu(conv2d(x, w_c1) + b_c1)\n",
    "   conv_1p = max_pool2x2(conv_1c)\n",
    "   conv_1d = tf.nn.dropout(conv_1p, keep_prob)\n",
    "   conv_2c = tf.nn.relu(conv2d(conv_1d, w_c2) + b_c2)\n",
    "   conv_2p = max_pool2x2(conv_2c)\n",
    "   conv_2d = tf.nn.dropout(conv_2p, keep_prob)\n",
    "   conv_3c = tf.nn.relu(conv2d(conv_2d, w_c3) + b_c3)\n",
    "   conv_3p = max_pool2x2(conv_3c)\n",
    "   conv_3d = tf.nn.dropout(conv_3p, keep_prob)\n",
    "   conv_3r = tf.reshape(conv_3d, [-1, full_connect_width * full_connect_height * 64])\n",
    "   fc_1 = tf.nn.relu(tf.matmul(conv_3r, w_d1) + b_d1)\n",
    "   fc_1d = tf.nn.dropout(fc_1, keep_prob)\n",
    "   out = tf.matmul(fc_1d, w_out) + b_out\n",
    "\n",
    "   return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-672d38f35e7c>:28: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/train_model-1400\n",
      "iteration：0 , batch_loss：0.3254881\n",
      "iteration：0 , acc：0.0925\n",
      "iteration：5 , batch_loss：0.32565576\n",
      "iteration：10 , batch_loss：0.32693452\n",
      "iteration：15 , batch_loss：0.32594782\n",
      "iteration：20 , batch_loss：0.32663256\n",
      "iteration：25 , batch_loss：0.32668543\n",
      "iteration：30 , batch_loss：0.32595006\n",
      "iteration：35 , batch_loss：0.32773232\n",
      "iteration：40 , batch_loss：0.32714808\n",
      "iteration：45 , batch_loss：0.3245657\n",
      "iteration：50 , batch_loss：0.32604808\n",
      "iteration：55 , batch_loss：0.3257455\n",
      "iteration：60 , batch_loss：0.326481\n",
      "iteration：65 , batch_loss：0.32604855\n",
      "iteration：70 , batch_loss：0.32696822\n",
      "iteration：75 , batch_loss：0.32764626\n",
      "iteration：80 , batch_loss：0.32621402\n",
      "iteration：85 , batch_loss：0.32729608\n",
      "iteration：90 , batch_loss：0.3274199\n",
      "iteration：95 , batch_loss：0.32776856\n",
      "iteration：100 , batch_loss：0.32687515\n",
      "iteration：100 , acc：0.0925\n",
      "iteration：105 , batch_loss：0.32558858\n",
      "iteration：110 , batch_loss：0.32718793\n",
      "iteration：115 , batch_loss：0.32548723\n",
      "iteration：120 , batch_loss：0.32608953\n",
      "iteration：125 , batch_loss：0.32852718\n",
      "iteration：130 , batch_loss：0.32653254\n",
      "iteration：135 , batch_loss：0.32693318\n",
      "iteration：140 , batch_loss：0.3262549\n",
      "iteration：145 , batch_loss：0.32537463\n",
      "iteration：150 , batch_loss：0.32574546\n",
      "iteration：155 , batch_loss：0.32674712\n",
      "iteration：160 , batch_loss：0.3245972\n",
      "iteration：165 , batch_loss：0.32775694\n",
      "iteration：170 , batch_loss：0.3262257\n",
      "iteration：175 , batch_loss：0.32650226\n",
      "iteration：180 , batch_loss：0.3250484\n",
      "iteration：185 , batch_loss：0.325252\n",
      "iteration：190 , batch_loss：0.32552695\n",
      "iteration：195 , batch_loss：0.32650432\n",
      "iteration：200 , batch_loss：0.32614455\n",
      "iteration：200 , acc：0.09\n",
      "iteration：205 , batch_loss：0.32924548\n",
      "iteration：210 , batch_loss：0.32553345\n",
      "iteration：215 , batch_loss：0.32697788\n",
      "iteration：220 , batch_loss：0.32691374\n",
      "iteration：225 , batch_loss：0.32414535\n",
      "iteration：230 , batch_loss：0.32585588\n",
      "iteration：235 , batch_loss：0.32675624\n",
      "iteration：240 , batch_loss：0.32624522\n",
      "iteration：245 , batch_loss：0.32666516\n",
      "iteration：250 , batch_loss：0.32621947\n",
      "iteration：255 , batch_loss：0.32499665\n",
      "iteration：260 , batch_loss：0.3276861\n",
      "iteration：265 , batch_loss：0.32604626\n",
      "iteration：270 , batch_loss：0.3256416\n",
      "iteration：275 , batch_loss：0.3257708\n",
      "iteration：280 , batch_loss：0.32650098\n",
      "iteration：285 , batch_loss：0.3260612\n",
      "iteration：290 , batch_loss：0.32669643\n",
      "iteration：295 , batch_loss：0.3259156\n",
      "iteration：300 , batch_loss：0.32632706\n",
      "iteration：300 , acc：0.1\n",
      "iteration：305 , batch_loss：0.32770407\n",
      "iteration：310 , batch_loss：0.32731256\n",
      "iteration：315 , batch_loss：0.32654962\n",
      "iteration：320 , batch_loss：0.3260395\n",
      "iteration：325 , batch_loss：0.32768703\n",
      "iteration：330 , batch_loss：0.32510155\n",
      "iteration：335 , batch_loss：0.3265236\n",
      "iteration：340 , batch_loss：0.32599798\n",
      "iteration：345 , batch_loss：0.32705697\n",
      "iteration：350 , batch_loss：0.32553294\n",
      "iteration：355 , batch_loss：0.32616192\n",
      "iteration：360 , batch_loss：0.32524362\n",
      "iteration：365 , batch_loss：0.3265776\n",
      "iteration：370 , batch_loss：0.32593837\n",
      "iteration：375 , batch_loss：0.32842618\n",
      "iteration：380 , batch_loss：0.32605228\n",
      "iteration：385 , batch_loss：0.32533428\n",
      "iteration：390 , batch_loss：0.32545328\n",
      "iteration：395 , batch_loss：0.32672253\n",
      "iteration：400 , batch_loss：0.3263115\n",
      "iteration：400 , acc：0.075\n",
      "iteration：405 , batch_loss：0.32557577\n",
      "iteration：410 , batch_loss：0.32573026\n",
      "iteration：415 , batch_loss：0.3256834\n",
      "iteration：420 , batch_loss：0.32626945\n",
      "iteration：425 , batch_loss：0.327254\n",
      "iteration：430 , batch_loss：0.32542008\n",
      "iteration：435 , batch_loss：0.32478398\n",
      "iteration：440 , batch_loss：0.32533413\n",
      "iteration：445 , batch_loss：0.328204\n",
      "iteration：450 , batch_loss：0.32580632\n",
      "iteration：455 , batch_loss：0.32553548\n",
      "iteration：460 , batch_loss：0.32528287\n",
      "iteration：465 , batch_loss：0.32596835\n",
      "iteration：470 , batch_loss：0.3258968\n",
      "iteration：475 , batch_loss：0.32473093\n",
      "iteration：480 , batch_loss：0.32526726\n",
      "iteration：485 , batch_loss：0.3257746\n",
      "iteration：490 , batch_loss：0.32590207\n",
      "iteration：495 , batch_loss：0.3263576\n",
      "iteration：500 , batch_loss：0.32496765\n",
      "WARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "iteration：500 , acc：0.095\n",
      "iteration：505 , batch_loss：0.32695383\n",
      "iteration：510 , batch_loss：0.3255261\n",
      "iteration：515 , batch_loss：0.32650366\n",
      "iteration：520 , batch_loss：0.3275681\n",
      "iteration：525 , batch_loss：0.32715413\n",
      "iteration：530 , batch_loss：0.32572627\n",
      "iteration：535 , batch_loss：0.32541576\n",
      "iteration：540 , batch_loss：0.32499748\n",
      "iteration：545 , batch_loss：0.32652193\n",
      "iteration：550 , batch_loss：0.32598218\n",
      "iteration：555 , batch_loss：0.32607192\n",
      "iteration：560 , batch_loss：0.32461894\n",
      "iteration：565 , batch_loss：0.32585388\n",
      "iteration：570 , batch_loss：0.3239521\n",
      "iteration：575 , batch_loss：0.32566926\n",
      "iteration：580 , batch_loss：0.3259442\n",
      "iteration：585 , batch_loss：0.3280226\n",
      "iteration：590 , batch_loss：0.32793254\n",
      "iteration：595 , batch_loss：0.32485357\n",
      "iteration：600 , batch_loss：0.32647955\n",
      "iteration：600 , acc：0.11\n",
      "iteration：605 , batch_loss：0.32696092\n",
      "iteration：610 , batch_loss：0.32488653\n",
      "iteration：615 , batch_loss：0.32514518\n",
      "iteration：620 , batch_loss：0.32527107\n",
      "iteration：625 , batch_loss：0.3269388\n",
      "iteration：630 , batch_loss：0.32530338\n",
      "iteration：635 , batch_loss：0.3267532\n",
      "iteration：640 , batch_loss：0.32786512\n",
      "iteration：645 , batch_loss：0.32586706\n",
      "iteration：650 , batch_loss：0.32605147\n",
      "iteration：655 , batch_loss：0.32652855\n",
      "iteration：660 , batch_loss：0.32525706\n",
      "iteration：665 , batch_loss：0.32624322\n",
      "iteration：670 , batch_loss：0.32728356\n",
      "iteration：675 , batch_loss：0.32573232\n",
      "iteration：680 , batch_loss：0.32472387\n",
      "iteration：685 , batch_loss：0.32682836\n",
      "iteration：690 , batch_loss：0.3266819\n",
      "iteration：695 , batch_loss：0.32809088\n",
      "iteration：700 , batch_loss：0.3261075\n",
      "iteration：700 , acc：0.1075\n",
      "iteration：705 , batch_loss：0.32594147\n",
      "iteration：710 , batch_loss：0.32661337\n",
      "iteration：715 , batch_loss：0.32522684\n",
      "iteration：720 , batch_loss：0.32537103\n",
      "iteration：725 , batch_loss：0.32580376\n",
      "iteration：730 , batch_loss：0.32676786\n",
      "iteration：735 , batch_loss：0.3242927\n",
      "iteration：740 , batch_loss：0.3276673\n",
      "iteration：745 , batch_loss：0.32772684\n",
      "iteration：750 , batch_loss：0.32471758\n",
      "iteration：755 , batch_loss：0.3254574\n",
      "iteration：760 , batch_loss：0.327119\n",
      "iteration：765 , batch_loss：0.3264967\n",
      "iteration：770 , batch_loss：0.32629004\n",
      "iteration：775 , batch_loss：0.3254977\n",
      "iteration：780 , batch_loss：0.3286035\n",
      "iteration：785 , batch_loss：0.32727674\n",
      "iteration：790 , batch_loss：0.3256139\n",
      "iteration：795 , batch_loss：0.32596108\n",
      "iteration：800 , batch_loss：0.32620087\n",
      "iteration：800 , acc：0.1075\n",
      "iteration：805 , batch_loss：0.32773843\n",
      "iteration：810 , batch_loss：0.3281713\n",
      "iteration：815 , batch_loss：0.32610804\n",
      "iteration：820 , batch_loss：0.3247227\n",
      "iteration：825 , batch_loss：0.32590955\n",
      "iteration：830 , batch_loss：0.32620043\n",
      "iteration：835 , batch_loss：0.32439178\n",
      "iteration：840 , batch_loss：0.32641682\n",
      "iteration：845 , batch_loss：0.32624012\n",
      "iteration：850 , batch_loss：0.32627386\n",
      "iteration：855 , batch_loss：0.32621163\n",
      "iteration：860 , batch_loss：0.32563198\n",
      "iteration：865 , batch_loss：0.32553834\n",
      "iteration：870 , batch_loss：0.3255643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration：875 , batch_loss：0.32630366\n",
      "iteration：880 , batch_loss：0.3274804\n",
      "iteration：885 , batch_loss：0.32793847\n",
      "iteration：890 , batch_loss：0.32846212\n",
      "iteration：895 , batch_loss：0.3281245\n",
      "iteration：900 , batch_loss：0.32645908\n",
      "iteration：900 , acc：0.0875\n",
      "iteration：905 , batch_loss：0.32662702\n",
      "iteration：910 , batch_loss：0.32622874\n",
      "iteration：915 , batch_loss：0.32642317\n",
      "iteration：920 , batch_loss：0.3254339\n",
      "iteration：925 , batch_loss：0.32642785\n",
      "iteration：930 , batch_loss：0.3270634\n",
      "iteration：935 , batch_loss：0.32639492\n",
      "iteration：940 , batch_loss：0.32585758\n",
      "iteration：945 , batch_loss：0.32558793\n",
      "iteration：950 , batch_loss：0.32570687\n",
      "iteration：955 , batch_loss：0.32638878\n",
      "iteration：960 , batch_loss：0.32488543\n",
      "iteration：965 , batch_loss：0.32515883\n",
      "iteration：970 , batch_loss：0.326144\n",
      "iteration：975 , batch_loss：0.32736138\n",
      "iteration：980 , batch_loss：0.32639226\n",
      "iteration：985 , batch_loss：0.32719728\n",
      "iteration：990 , batch_loss：0.325356\n",
      "iteration：995 , batch_loss：0.32659176\n",
      "iteration：1000 , batch_loss：0.32637602\n",
      "iteration：1000 , acc：0.1025\n",
      "iteration：1005 , batch_loss：0.3273886\n",
      "iteration：1010 , batch_loss：0.3263646\n",
      "iteration：1015 , batch_loss：0.32447058\n",
      "iteration：1020 , batch_loss：0.3271129\n",
      "iteration：1025 , batch_loss：0.3260707\n",
      "iteration：1030 , batch_loss：0.32648668\n",
      "iteration：1035 , batch_loss：0.32674104\n",
      "iteration：1040 , batch_loss：0.32552385\n",
      "iteration：1045 , batch_loss：0.3261854\n",
      "iteration：1050 , batch_loss：0.32689852\n",
      "iteration：1055 , batch_loss：0.32480544\n",
      "iteration：1060 , batch_loss：0.3253764\n",
      "iteration：1065 , batch_loss：0.32621628\n",
      "iteration：1070 , batch_loss：0.32709518\n",
      "iteration：1075 , batch_loss：0.32497942\n",
      "iteration：1080 , batch_loss：0.32644802\n",
      "iteration：1085 , batch_loss：0.32659826\n",
      "iteration：1090 , batch_loss：0.32605967\n",
      "iteration：1095 , batch_loss：0.32458267\n",
      "iteration：1100 , batch_loss：0.32624918\n",
      "iteration：1100 , acc：0.1275\n",
      "iteration：1105 , batch_loss：0.32545048\n",
      "iteration：1110 , batch_loss：0.32464415\n",
      "iteration：1115 , batch_loss：0.32786283\n",
      "iteration：1120 , batch_loss：0.32551348\n",
      "iteration：1125 , batch_loss：0.32593745\n",
      "iteration：1130 , batch_loss：0.32419413\n",
      "iteration：1135 , batch_loss：0.32763764\n",
      "iteration：1140 , batch_loss：0.32430458\n",
      "iteration：1145 , batch_loss：0.3265377\n",
      "iteration：1150 , batch_loss：0.32668015\n",
      "iteration：1155 , batch_loss：0.32522446\n",
      "iteration：1160 , batch_loss：0.32751042\n",
      "iteration：1165 , batch_loss：0.32670006\n",
      "iteration：1170 , batch_loss：0.32575715\n",
      "iteration：1175 , batch_loss：0.32564026\n",
      "iteration：1180 , batch_loss：0.32644206\n",
      "iteration：1185 , batch_loss：0.32602853\n",
      "iteration：1190 , batch_loss：0.3268208\n",
      "iteration：1195 , batch_loss：0.32567748\n",
      "iteration：1200 , batch_loss：0.3241764\n",
      "iteration：1200 , acc：0.11\n",
      "iteration：1205 , batch_loss：0.32544962\n",
      "iteration：1210 , batch_loss：0.32654163\n",
      "iteration：1215 , batch_loss：0.32632732\n",
      "iteration：1220 , batch_loss：0.3255964\n",
      "iteration：1225 , batch_loss：0.32604194\n",
      "iteration：1230 , batch_loss：0.3259882\n",
      "iteration：1235 , batch_loss：0.32545742\n",
      "iteration：1240 , batch_loss：0.3260904\n",
      "iteration：1245 , batch_loss：0.3252744\n",
      "iteration：1250 , batch_loss：0.32572928\n",
      "iteration：1255 , batch_loss：0.32667914\n",
      "iteration：1260 , batch_loss：0.3258664\n",
      "iteration：1265 , batch_loss：0.3268177\n",
      "iteration：1270 , batch_loss：0.3249499\n",
      "iteration：1275 , batch_loss：0.32531238\n",
      "iteration：1280 , batch_loss：0.32655913\n",
      "iteration：1285 , batch_loss：0.3253714\n",
      "iteration：1290 , batch_loss：0.3267836\n",
      "iteration：1295 , batch_loss：0.3264634\n",
      "iteration：1300 , batch_loss：0.32664675\n",
      "iteration：1300 , acc：0.105\n",
      "iteration：1305 , batch_loss：0.32681322\n",
      "iteration：1310 , batch_loss：0.32596993\n",
      "iteration：1315 , batch_loss：0.32877037\n",
      "iteration：1320 , batch_loss：0.3266605\n",
      "iteration：1325 , batch_loss：0.3257854\n",
      "iteration：1330 , batch_loss：0.32469925\n",
      "iteration：1335 , batch_loss：0.3259082\n",
      "iteration：1340 , batch_loss：0.3261358\n",
      "iteration：1345 , batch_loss：0.32546416\n",
      "iteration：1350 , batch_loss：0.3270186\n",
      "iteration：1355 , batch_loss：0.32602695\n",
      "iteration：1360 , batch_loss：0.3257247\n",
      "iteration：1365 , batch_loss：0.3273867\n",
      "iteration：1370 , batch_loss：0.3256887\n",
      "iteration：1375 , batch_loss：0.32779276\n",
      "iteration：1380 , batch_loss：0.32707572\n",
      "iteration：1385 , batch_loss：0.32699466\n",
      "iteration：1390 , batch_loss：0.32464084\n",
      "iteration：1395 , batch_loss：0.32556552\n",
      "iteration：1400 , batch_loss：0.32619348\n",
      "iteration：1400 , acc：0.1225\n",
      "iteration：1405 , batch_loss：0.32618394\n",
      "iteration：1410 , batch_loss：0.32532483\n",
      "iteration：1415 , batch_loss：0.32654315\n",
      "iteration：1420 , batch_loss：0.3255546\n",
      "iteration：1425 , batch_loss：0.3267469\n",
      "iteration：1430 , batch_loss：0.3257901\n",
      "iteration：1435 , batch_loss：0.32512206\n",
      "iteration：1440 , batch_loss：0.3265347\n",
      "iteration：1445 , batch_loss：0.32623816\n",
      "iteration：1450 , batch_loss：0.326757\n",
      "iteration：1455 , batch_loss：0.32517424\n",
      "iteration：1460 , batch_loss：0.32535714\n",
      "iteration：1465 , batch_loss：0.32700238\n",
      "iteration：1470 , batch_loss：0.32598776\n",
      "iteration：1475 , batch_loss：0.32697862\n",
      "iteration：1480 , batch_loss：0.32632145\n",
      "iteration：1485 , batch_loss：0.32516804\n",
      "iteration：1490 , batch_loss：0.3256859\n",
      "iteration：1495 , batch_loss：0.32618824\n",
      "iteration：1500 , batch_loss：0.325862\n",
      "iteration：1500 , acc：0.0875\n",
      "iteration：1505 , batch_loss：0.3260271\n",
      "iteration：1510 , batch_loss：0.32523385\n",
      "iteration：1515 , batch_loss：0.3260936\n",
      "iteration：1520 , batch_loss：0.326026\n",
      "iteration：1525 , batch_loss：0.32488394\n",
      "iteration：1530 , batch_loss：0.32622063\n",
      "iteration：1535 , batch_loss：0.32541496\n",
      "iteration：1540 , batch_loss：0.32477513\n",
      "iteration：1545 , batch_loss：0.32804927\n",
      "iteration：1550 , batch_loss：0.32558435\n",
      "iteration：1555 , batch_loss：0.32644367\n",
      "iteration：1560 , batch_loss：0.32536075\n",
      "iteration：1565 , batch_loss：0.3258615\n",
      "iteration：1570 , batch_loss：0.32633314\n",
      "iteration：1575 , batch_loss：0.32491928\n",
      "iteration：1580 , batch_loss：0.32507268\n",
      "iteration：1585 , batch_loss：0.3257667\n",
      "iteration：1590 , batch_loss：0.32654625\n",
      "iteration：1595 , batch_loss：0.32709378\n",
      "iteration：1600 , batch_loss：0.3258412\n",
      "iteration：1600 , acc：0.0975\n",
      "iteration：1605 , batch_loss：0.32604092\n",
      "iteration：1610 , batch_loss：0.3262536\n",
      "iteration：1615 , batch_loss：0.32538888\n",
      "iteration：1620 , batch_loss：0.32643634\n",
      "iteration：1625 , batch_loss：0.32598895\n",
      "iteration：1630 , batch_loss：0.3254432\n",
      "iteration：1635 , batch_loss：0.32541147\n",
      "iteration：1640 , batch_loss：0.32567713\n",
      "iteration：1645 , batch_loss：0.32578397\n",
      "iteration：1650 , batch_loss：0.3263065\n",
      "iteration：1655 , batch_loss：0.32659048\n",
      "iteration：1660 , batch_loss：0.32547277\n",
      "iteration：1665 , batch_loss：0.32639182\n",
      "iteration：1670 , batch_loss：0.32709843\n",
      "iteration：1675 , batch_loss：0.3246532\n",
      "iteration：1680 , batch_loss：0.3262001\n",
      "iteration：1685 , batch_loss：0.3268721\n",
      "iteration：1690 , batch_loss：0.32624045\n",
      "iteration：1695 , batch_loss：0.32561904\n",
      "iteration：1700 , batch_loss：0.3239525\n",
      "iteration：1700 , acc：0.0975\n",
      "iteration：1705 , batch_loss：0.3271737\n",
      "iteration：1710 , batch_loss：0.32481128\n",
      "iteration：1715 , batch_loss：0.3268842\n",
      "iteration：1720 , batch_loss：0.32516629\n",
      "iteration：1725 , batch_loss：0.3264931\n",
      "iteration：1730 , batch_loss：0.32508045\n",
      "iteration：1735 , batch_loss：0.32431024\n",
      "iteration：1740 , batch_loss：0.32642713\n",
      "iteration：1745 , batch_loss：0.325004\n",
      "iteration：1750 , batch_loss：0.32641056\n",
      "iteration：1755 , batch_loss：0.32631356\n",
      "iteration：1760 , batch_loss：0.32680786\n",
      "iteration：1765 , batch_loss：0.32690448\n",
      "iteration：1770 , batch_loss：0.327497\n",
      "iteration：1775 , batch_loss：0.32765922\n",
      "iteration：1780 , batch_loss：0.32671243\n",
      "iteration：1785 , batch_loss：0.3266649\n",
      "iteration：1790 , batch_loss：0.32621127\n",
      "iteration：1795 , batch_loss：0.32657835\n",
      "iteration：1800 , batch_loss：0.3262619\n",
      "iteration：1800 , acc：0.12\n",
      "iteration：1805 , batch_loss：0.32535234\n",
      "iteration：1810 , batch_loss：0.32739514\n",
      "iteration：1815 , batch_loss：0.32543075\n",
      "iteration：1820 , batch_loss：0.32571203\n",
      "iteration：1825 , batch_loss：0.32579476\n",
      "iteration：1830 , batch_loss：0.32526183\n",
      "iteration：1835 , batch_loss：0.3240946\n",
      "iteration：1840 , batch_loss：0.32558027\n",
      "iteration：1845 , batch_loss：0.32622233\n",
      "iteration：1850 , batch_loss：0.32829678\n",
      "iteration：1855 , batch_loss：0.32668585\n",
      "iteration：1860 , batch_loss：0.3272403\n",
      "iteration：1865 , batch_loss：0.32497972\n",
      "iteration：1870 , batch_loss：0.324596\n",
      "iteration：1875 , batch_loss：0.32471332\n",
      "iteration：1880 , batch_loss：0.32577422\n",
      "iteration：1885 , batch_loss：0.32631427\n",
      "iteration：1890 , batch_loss：0.32735565\n",
      "iteration：1895 , batch_loss：0.3275686\n",
      "iteration：1900 , batch_loss：0.32587323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration：1900 , acc：0.11\n",
      "iteration：1905 , batch_loss：0.32637626\n",
      "iteration：1910 , batch_loss：0.32562\n",
      "iteration：1915 , batch_loss：0.32619825\n",
      "iteration：1920 , batch_loss：0.326873\n",
      "iteration：1925 , batch_loss：0.3260923\n",
      "iteration：1930 , batch_loss：0.32458416\n",
      "iteration：1935 , batch_loss：0.32419753\n",
      "iteration：1940 , batch_loss：0.32716107\n",
      "iteration：1945 , batch_loss：0.32723933\n",
      "iteration：1950 , batch_loss：0.32459918\n",
      "iteration：1955 , batch_loss：0.32482862\n",
      "iteration：1960 , batch_loss：0.32682815\n",
      "iteration：1965 , batch_loss：0.32643276\n",
      "iteration：1970 , batch_loss：0.32703218\n",
      "iteration：1975 , batch_loss：0.32596207\n",
      "iteration：1980 , batch_loss：0.32622743\n",
      "iteration：1985 , batch_loss：0.32633573\n",
      "iteration：1990 , batch_loss：0.3264009\n",
      "iteration：1995 , batch_loss：0.32419655\n",
      "iteration：2000 , batch_loss：0.3244051\n",
      "iteration：2000 , acc：0.0775\n",
      "iteration：2005 , batch_loss：0.325827\n",
      "iteration：2010 , batch_loss：0.3248014\n",
      "iteration：2015 , batch_loss：0.32560974\n",
      "iteration：2020 , batch_loss：0.32673582\n",
      "iteration：2025 , batch_loss：0.3264912\n",
      "iteration：2030 , batch_loss：0.32652396\n",
      "iteration：2035 , batch_loss：0.3264649\n",
      "iteration：2040 , batch_loss：0.32658297\n",
      "iteration：2045 , batch_loss：0.3265978\n",
      "iteration：2050 , batch_loss：0.32636476\n",
      "iteration：2055 , batch_loss：0.32511526\n",
      "iteration：2060 , batch_loss：0.32447085\n",
      "iteration：2065 , batch_loss：0.32682288\n",
      "iteration：2070 , batch_loss：0.32598397\n",
      "iteration：2075 , batch_loss：0.325717\n",
      "iteration：2080 , batch_loss：0.3265744\n",
      "iteration：2085 , batch_loss：0.32559714\n",
      "iteration：2090 , batch_loss：0.3266676\n",
      "iteration：2095 , batch_loss：0.32568234\n",
      "iteration：2100 , batch_loss：0.32546696\n",
      "iteration：2100 , acc：0.0725\n",
      "iteration：2105 , batch_loss：0.32636294\n",
      "iteration：2110 , batch_loss：0.32650614\n",
      "iteration：2115 , batch_loss：0.32529336\n",
      "iteration：2120 , batch_loss：0.32572156\n",
      "iteration：2125 , batch_loss：0.32637343\n",
      "iteration：2130 , batch_loss：0.3265423\n",
      "iteration：2135 , batch_loss：0.3255857\n",
      "iteration：2140 , batch_loss：0.32651085\n",
      "iteration：2145 , batch_loss：0.32412177\n",
      "iteration：2150 , batch_loss：0.32678562\n",
      "iteration：2155 , batch_loss：0.32491478\n",
      "iteration：2160 , batch_loss：0.32599467\n",
      "iteration：2165 , batch_loss：0.32573718\n",
      "iteration：2170 , batch_loss：0.326441\n",
      "iteration：2175 , batch_loss：0.3254779\n",
      "iteration：2180 , batch_loss：0.32675606\n",
      "iteration：2185 , batch_loss：0.32582176\n",
      "iteration：2190 , batch_loss：0.32589355\n",
      "iteration：2195 , batch_loss：0.32483798\n",
      "iteration：2200 , batch_loss：0.32605934\n",
      "iteration：2200 , acc：0.1225\n",
      "iteration：2205 , batch_loss：0.32494214\n",
      "iteration：2210 , batch_loss：0.32585937\n",
      "iteration：2215 , batch_loss：0.32667524\n",
      "iteration：2220 , batch_loss：0.3258427\n",
      "iteration：2225 , batch_loss：0.32705647\n",
      "iteration：2230 , batch_loss：0.3257882\n",
      "iteration：2235 , batch_loss：0.3252271\n",
      "iteration：2240 , batch_loss：0.32453638\n",
      "iteration：2245 , batch_loss：0.3244384\n",
      "iteration：2250 , batch_loss：0.32573268\n",
      "iteration：2255 , batch_loss：0.32569206\n",
      "iteration：2260 , batch_loss：0.32633108\n",
      "iteration：2265 , batch_loss：0.3261829\n",
      "iteration：2270 , batch_loss：0.326351\n",
      "iteration：2275 , batch_loss：0.3268825\n",
      "iteration：2280 , batch_loss：0.32601988\n",
      "iteration：2285 , batch_loss：0.3252755\n",
      "iteration：2290 , batch_loss：0.32611138\n",
      "iteration：2295 , batch_loss：0.32533333\n",
      "iteration：2300 , batch_loss：0.32556504\n",
      "iteration：2300 , acc：0.1025\n",
      "iteration：2305 , batch_loss：0.32542402\n",
      "iteration：2310 , batch_loss：0.32669702\n",
      "iteration：2315 , batch_loss：0.32563525\n",
      "iteration：2320 , batch_loss：0.32660127\n",
      "iteration：2325 , batch_loss：0.32493278\n",
      "iteration：2330 , batch_loss：0.32765955\n",
      "iteration：2335 , batch_loss：0.32478395\n",
      "iteration：2340 , batch_loss：0.3250082\n",
      "iteration：2345 , batch_loss：0.32491177\n",
      "iteration：2350 , batch_loss：0.32652417\n",
      "iteration：2355 , batch_loss：0.32507455\n",
      "iteration：2360 , batch_loss：0.32551378\n",
      "iteration：2365 , batch_loss：0.32518715\n",
      "iteration：2370 , batch_loss：0.32553583\n",
      "iteration：2375 , batch_loss：0.32538724\n",
      "iteration：2380 , batch_loss：0.32611153\n",
      "iteration：2385 , batch_loss：0.3260954\n",
      "iteration：2390 , batch_loss：0.32576072\n",
      "iteration：2395 , batch_loss：0.32439262\n",
      "iteration：2400 , batch_loss：0.3262326\n",
      "iteration：2400 , acc：0.0875\n",
      "iteration：2405 , batch_loss：0.32555538\n",
      "iteration：2410 , batch_loss：0.3254308\n",
      "iteration：2415 , batch_loss：0.32591915\n",
      "iteration：2420 , batch_loss：0.32686782\n",
      "iteration：2425 , batch_loss：0.3257958\n",
      "iteration：2430 , batch_loss：0.3256599\n",
      "iteration：2435 , batch_loss：0.3269299\n",
      "iteration：2440 , batch_loss：0.32550138\n",
      "iteration：2445 , batch_loss：0.3261784\n",
      "iteration：2450 , batch_loss：0.32580465\n",
      "iteration：2455 , batch_loss：0.32676664\n",
      "iteration：2460 , batch_loss：0.32637766\n",
      "iteration：2465 , batch_loss：0.32684216\n",
      "iteration：2470 , batch_loss：0.32416567\n",
      "iteration：2475 , batch_loss：0.32711712\n",
      "iteration：2480 , batch_loss：0.3260762\n",
      "iteration：2485 , batch_loss：0.32584304\n",
      "iteration：2490 , batch_loss：0.32617372\n",
      "iteration：2495 , batch_loss：0.32587677\n",
      "iteration：2500 , batch_loss：0.32681608\n",
      "iteration：2500 , acc：0.11\n",
      "iteration：2505 , batch_loss：0.32644302\n",
      "iteration：2510 , batch_loss：0.3257007\n",
      "iteration：2515 , batch_loss：0.32668748\n",
      "iteration：2520 , batch_loss：0.32670492\n",
      "iteration：2525 , batch_loss：0.32643756\n",
      "iteration：2530 , batch_loss：0.3256169\n",
      "iteration：2535 , batch_loss：0.32579154\n",
      "iteration：2540 , batch_loss：0.32474732\n",
      "iteration：2545 , batch_loss：0.32622766\n",
      "iteration：2550 , batch_loss：0.32612666\n",
      "iteration：2555 , batch_loss：0.32672048\n",
      "iteration：2560 , batch_loss：0.32537168\n",
      "iteration：2565 , batch_loss：0.3269263\n",
      "iteration：2570 , batch_loss：0.3249914\n",
      "iteration：2575 , batch_loss：0.32591635\n",
      "iteration：2580 , batch_loss：0.32448405\n",
      "iteration：2585 , batch_loss：0.3260326\n",
      "iteration：2590 , batch_loss：0.3255525\n",
      "iteration：2595 , batch_loss：0.32521582\n",
      "iteration：2600 , batch_loss：0.3260514\n",
      "iteration：2600 , acc：0.1\n",
      "iteration：2605 , batch_loss：0.32535222\n",
      "iteration：2610 , batch_loss：0.32522964\n",
      "iteration：2615 , batch_loss：0.32504824\n",
      "iteration：2620 , batch_loss：0.32517362\n",
      "iteration：2625 , batch_loss：0.32526356\n",
      "iteration：2630 , batch_loss：0.32615513\n",
      "iteration：2635 , batch_loss：0.3256613\n",
      "iteration：2640 , batch_loss：0.32720137\n",
      "iteration：2645 , batch_loss：0.32604825\n",
      "iteration：2650 , batch_loss：0.32621336\n",
      "iteration：2655 , batch_loss：0.3264218\n",
      "iteration：2660 , batch_loss：0.32543403\n",
      "iteration：2665 , batch_loss：0.326016\n",
      "iteration：2670 , batch_loss：0.32643706\n",
      "iteration：2675 , batch_loss：0.32631722\n",
      "iteration：2680 , batch_loss：0.3257765\n",
      "iteration：2685 , batch_loss：0.32586616\n",
      "iteration：2690 , batch_loss：0.32689795\n",
      "iteration：2695 , batch_loss：0.3257539\n",
      "iteration：2700 , batch_loss：0.3261404\n",
      "iteration：2700 , acc：0.095\n",
      "iteration：2705 , batch_loss：0.3252377\n",
      "iteration：2710 , batch_loss：0.3263889\n",
      "iteration：2715 , batch_loss：0.32653922\n",
      "iteration：2720 , batch_loss：0.32534394\n",
      "iteration：2725 , batch_loss：0.3257726\n",
      "iteration：2730 , batch_loss：0.32589325\n",
      "iteration：2735 , batch_loss：0.32612807\n",
      "iteration：2740 , batch_loss：0.32582384\n",
      "iteration：2745 , batch_loss：0.3248573\n",
      "iteration：2750 , batch_loss：0.3264833\n",
      "iteration：2755 , batch_loss：0.3258567\n",
      "iteration：2760 , batch_loss：0.3269522\n",
      "iteration：2765 , batch_loss：0.32585388\n",
      "iteration：2770 , batch_loss：0.32713127\n",
      "iteration：2775 , batch_loss：0.32633084\n",
      "iteration：2780 , batch_loss：0.32753807\n",
      "iteration：2785 , batch_loss：0.3254859\n",
      "iteration：2790 , batch_loss：0.32628623\n",
      "iteration：2795 , batch_loss：0.3259631\n",
      "iteration：2800 , batch_loss：0.32640606\n",
      "iteration：2800 , acc：0.1025\n",
      "iteration：2805 , batch_loss：0.32592335\n",
      "iteration：2810 , batch_loss：0.3268311\n",
      "iteration：2815 , batch_loss：0.32620448\n",
      "iteration：2820 , batch_loss：0.32632867\n",
      "iteration：2825 , batch_loss：0.3254873\n",
      "iteration：2830 , batch_loss：0.3259647\n",
      "iteration：2835 , batch_loss：0.32636222\n",
      "iteration：2840 , batch_loss：0.32554194\n",
      "iteration：2845 , batch_loss：0.32701087\n",
      "iteration：2850 , batch_loss：0.32561356\n",
      "iteration：2855 , batch_loss：0.3250141\n",
      "iteration：2860 , batch_loss：0.32608852\n",
      "iteration：2865 , batch_loss：0.3255419\n",
      "iteration：2870 , batch_loss：0.32614487\n",
      "iteration：2875 , batch_loss：0.32653186\n",
      "iteration：2880 , batch_loss：0.32660753\n",
      "iteration：2885 , batch_loss：0.32689923\n",
      "iteration：2890 , batch_loss：0.3255409\n",
      "iteration：2895 , batch_loss：0.32540303\n",
      "iteration：2900 , batch_loss：0.32759303\n",
      "iteration：2900 , acc：0.1125\n",
      "iteration：2905 , batch_loss：0.32482323\n",
      "iteration：2910 , batch_loss：0.3265233\n",
      "iteration：2915 , batch_loss：0.32548338\n",
      "iteration：2920 , batch_loss：0.32562786\n",
      "iteration：2925 , batch_loss：0.3264597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration：2930 , batch_loss：0.3255717\n",
      "iteration：2935 , batch_loss：0.32575592\n",
      "iteration：2940 , batch_loss：0.32394275\n",
      "iteration：2945 , batch_loss：0.32641813\n",
      "iteration：2950 , batch_loss：0.3255481\n",
      "iteration：2955 , batch_loss：0.32692537\n",
      "iteration：2960 , batch_loss：0.32575333\n",
      "iteration：2965 , batch_loss：0.32457542\n",
      "iteration：2970 , batch_loss：0.32581982\n",
      "iteration：2975 , batch_loss：0.32504502\n",
      "iteration：2980 , batch_loss：0.32563144\n",
      "iteration：2985 , batch_loss：0.32738155\n",
      "iteration：2990 , batch_loss：0.32658225\n",
      "iteration：2995 , batch_loss：0.32464814\n",
      "iteration：3000 , batch_loss：0.325537\n",
      "iteration：3000 , acc：0.1025\n",
      "iteration：3005 , batch_loss：0.3268074\n",
      "iteration：3010 , batch_loss：0.32600254\n",
      "iteration：3015 , batch_loss：0.3246504\n",
      "iteration：3020 , batch_loss：0.32548395\n",
      "iteration：3025 , batch_loss：0.3258376\n",
      "iteration：3030 , batch_loss：0.32595608\n",
      "iteration：3035 , batch_loss：0.32681113\n",
      "iteration：3040 , batch_loss：0.32493824\n",
      "iteration：3045 , batch_loss：0.32480302\n",
      "iteration：3050 , batch_loss：0.32415062\n",
      "iteration：3055 , batch_loss：0.32582575\n",
      "iteration：3060 , batch_loss：0.32704777\n",
      "iteration：3065 , batch_loss：0.32566127\n",
      "iteration：3070 , batch_loss：0.32566756\n",
      "iteration：3075 , batch_loss：0.32625842\n",
      "iteration：3080 , batch_loss：0.32624078\n",
      "iteration：3085 , batch_loss：0.3249667\n",
      "iteration：3090 , batch_loss：0.32498112\n",
      "iteration：3095 , batch_loss：0.3259589\n",
      "iteration：3100 , batch_loss：0.32531333\n",
      "iteration：3100 , acc：0.095\n",
      "iteration：3105 , batch_loss：0.3270735\n",
      "iteration：3110 , batch_loss：0.3261613\n",
      "iteration：3115 , batch_loss：0.32543543\n",
      "iteration：3120 , batch_loss：0.32493138\n",
      "iteration：3125 , batch_loss：0.32649824\n",
      "iteration：3130 , batch_loss：0.32614282\n",
      "iteration：3135 , batch_loss：0.32581568\n",
      "iteration：3140 , batch_loss：0.3254809\n",
      "iteration：3145 , batch_loss：0.32622823\n",
      "iteration：3150 , batch_loss：0.32516927\n",
      "iteration：3155 , batch_loss：0.32457757\n",
      "iteration：3160 , batch_loss：0.3250435\n",
      "iteration：3165 , batch_loss：0.3251966\n",
      "iteration：3170 , batch_loss：0.32635766\n",
      "iteration：3175 , batch_loss：0.32604986\n",
      "iteration：3180 , batch_loss：0.3260091\n",
      "iteration：3185 , batch_loss：0.32532918\n",
      "iteration：3190 , batch_loss：0.3256016\n",
      "iteration：3195 , batch_loss：0.3260348\n",
      "iteration：3200 , batch_loss：0.32651174\n",
      "iteration：3200 , acc：0.1175\n",
      "iteration：3205 , batch_loss：0.3265667\n",
      "iteration：3210 , batch_loss：0.32609835\n",
      "iteration：3215 , batch_loss：0.32588518\n",
      "iteration：3220 , batch_loss：0.3260446\n",
      "iteration：3225 , batch_loss：0.32525435\n",
      "iteration：3230 , batch_loss：0.32607937\n",
      "iteration：3235 , batch_loss：0.3247133\n",
      "iteration：3240 , batch_loss：0.32594424\n",
      "iteration：3245 , batch_loss：0.326096\n",
      "iteration：3250 , batch_loss：0.32731527\n",
      "iteration：3255 , batch_loss：0.32552856\n",
      "iteration：3260 , batch_loss：0.32513317\n",
      "iteration：3265 , batch_loss：0.32524854\n",
      "iteration：3270 , batch_loss：0.32543358\n",
      "iteration：3275 , batch_loss：0.32569823\n",
      "iteration：3280 , batch_loss：0.3259477\n",
      "iteration：3285 , batch_loss：0.3271324\n",
      "iteration：3290 , batch_loss：0.3240969\n",
      "iteration：3295 , batch_loss：0.32677978\n",
      "iteration：3300 , batch_loss：0.3262846\n",
      "iteration：3300 , acc：0.115\n",
      "iteration：3305 , batch_loss：0.32596275\n",
      "iteration：3310 , batch_loss：0.3263113\n",
      "iteration：3315 , batch_loss：0.3244087\n",
      "iteration：3320 , batch_loss：0.32545772\n",
      "iteration：3325 , batch_loss：0.32652783\n",
      "iteration：3330 , batch_loss：0.32585445\n",
      "iteration：3335 , batch_loss：0.32591218\n",
      "iteration：3340 , batch_loss：0.32677582\n",
      "iteration：3345 , batch_loss：0.325018\n",
      "iteration：3350 , batch_loss：0.3250248\n",
      "iteration：3355 , batch_loss：0.32535928\n",
      "iteration：3360 , batch_loss：0.32470703\n",
      "iteration：3365 , batch_loss：0.32478684\n",
      "iteration：3370 , batch_loss：0.32631513\n",
      "iteration：3375 , batch_loss：0.32582456\n",
      "iteration：3380 , batch_loss：0.32666296\n",
      "iteration：3385 , batch_loss：0.32527274\n",
      "iteration：3390 , batch_loss：0.3257956\n",
      "iteration：3395 , batch_loss：0.32680577\n",
      "iteration：3400 , batch_loss：0.32460347\n",
      "iteration：3400 , acc：0.0825\n",
      "iteration：3405 , batch_loss：0.32528883\n",
      "iteration：3410 , batch_loss：0.32584465\n",
      "iteration：3415 , batch_loss：0.3252855\n",
      "iteration：3420 , batch_loss：0.32426754\n",
      "iteration：3425 , batch_loss：0.32753533\n",
      "iteration：3430 , batch_loss：0.32609105\n",
      "iteration：3435 , batch_loss：0.32603613\n",
      "iteration：3440 , batch_loss：0.3251927\n",
      "iteration：3445 , batch_loss：0.32512328\n",
      "iteration：3450 , batch_loss：0.32623026\n",
      "iteration：3455 , batch_loss：0.32528415\n",
      "iteration：3460 , batch_loss：0.32494754\n",
      "iteration：3465 , batch_loss：0.32438597\n",
      "iteration：3470 , batch_loss：0.3255289\n",
      "iteration：3475 , batch_loss：0.3255236\n",
      "iteration：3480 , batch_loss：0.32618877\n",
      "iteration：3485 , batch_loss：0.32511315\n",
      "iteration：3490 , batch_loss：0.32680207\n",
      "iteration：3495 , batch_loss：0.3253905\n",
      "iteration：3500 , batch_loss：0.32573014\n",
      "iteration：3500 , acc：0.09\n",
      "iteration：3505 , batch_loss：0.32531267\n",
      "iteration：3510 , batch_loss：0.32475826\n",
      "iteration：3515 , batch_loss：0.3251435\n",
      "iteration：3520 , batch_loss：0.32563978\n",
      "iteration：3525 , batch_loss：0.32587895\n",
      "iteration：3530 , batch_loss：0.32663766\n",
      "iteration：3535 , batch_loss：0.32473955\n",
      "iteration：3540 , batch_loss：0.32573968\n",
      "iteration：3545 , batch_loss：0.32576656\n",
      "iteration：3550 , batch_loss：0.3265167\n",
      "iteration：3555 , batch_loss：0.32706267\n",
      "iteration：3560 , batch_loss：0.32586217\n",
      "iteration：3565 , batch_loss：0.32577786\n",
      "iteration：3570 , batch_loss：0.3258858\n",
      "iteration：3575 , batch_loss：0.32590064\n",
      "iteration：3580 , batch_loss：0.32547322\n",
      "iteration：3585 , batch_loss：0.32631811\n",
      "iteration：3590 , batch_loss：0.32639462\n",
      "iteration：3595 , batch_loss：0.3262784\n",
      "iteration：3600 , batch_loss：0.32592446\n",
      "iteration：3600 , acc：0.1\n",
      "iteration：3605 , batch_loss：0.32651696\n",
      "iteration：3610 , batch_loss：0.32590634\n",
      "iteration：3615 , batch_loss：0.3257108\n",
      "iteration：3620 , batch_loss：0.32487464\n",
      "iteration：3625 , batch_loss：0.32550898\n",
      "iteration：3630 , batch_loss：0.32619292\n",
      "iteration：3635 , batch_loss：0.32653219\n",
      "iteration：3640 , batch_loss：0.32524768\n",
      "iteration：3645 , batch_loss：0.32600918\n",
      "iteration：3650 , batch_loss：0.32676855\n",
      "iteration：3655 , batch_loss：0.32675236\n",
      "iteration：3660 , batch_loss：0.3268279\n",
      "iteration：3665 , batch_loss：0.32540163\n",
      "iteration：3670 , batch_loss：0.32459825\n",
      "iteration：3675 , batch_loss：0.3269584\n",
      "iteration：3680 , batch_loss：0.32583603\n",
      "iteration：3685 , batch_loss：0.32685\n",
      "iteration：3690 , batch_loss：0.32500643\n",
      "iteration：3695 , batch_loss：0.3250546\n",
      "iteration：3700 , batch_loss：0.3254531\n",
      "iteration：3700 , acc：0.1275\n",
      "iteration：3705 , batch_loss：0.3255785\n",
      "iteration：3710 , batch_loss：0.32574886\n",
      "iteration：3715 , batch_loss：0.3249218\n",
      "iteration：3720 , batch_loss：0.3272168\n",
      "iteration：3725 , batch_loss：0.32608062\n",
      "iteration：3730 , batch_loss：0.32550174\n",
      "iteration：3735 , batch_loss：0.32552224\n",
      "iteration：3740 , batch_loss：0.32518882\n",
      "iteration：3745 , batch_loss：0.32579643\n",
      "iteration：3750 , batch_loss：0.326013\n",
      "iteration：3755 , batch_loss：0.32492775\n",
      "iteration：3760 , batch_loss：0.32522282\n",
      "iteration：3765 , batch_loss：0.32697153\n",
      "iteration：3770 , batch_loss：0.3254936\n",
      "iteration：3775 , batch_loss：0.32592738\n",
      "iteration：3780 , batch_loss：0.32595557\n",
      "iteration：3785 , batch_loss：0.32577997\n",
      "iteration：3790 , batch_loss：0.32534322\n",
      "iteration：3795 , batch_loss：0.32532382\n",
      "iteration：3800 , batch_loss：0.3259357\n",
      "iteration：3800 , acc：0.105\n",
      "iteration：3805 , batch_loss：0.32704148\n",
      "iteration：3810 , batch_loss：0.32505187\n",
      "iteration：3815 , batch_loss：0.32530868\n",
      "iteration：3820 , batch_loss：0.32562834\n",
      "iteration：3825 , batch_loss：0.32648307\n",
      "iteration：3830 , batch_loss：0.32736975\n",
      "iteration：3835 , batch_loss：0.32570344\n",
      "iteration：3840 , batch_loss：0.32581747\n",
      "iteration：3845 , batch_loss：0.32661167\n",
      "iteration：3850 , batch_loss：0.32601684\n",
      "iteration：3855 , batch_loss：0.32624477\n",
      "iteration：3860 , batch_loss：0.32493284\n",
      "iteration：3865 , batch_loss：0.32573786\n",
      "iteration：3870 , batch_loss：0.32599613\n",
      "iteration：3875 , batch_loss：0.3241047\n",
      "iteration：3880 , batch_loss：0.32700533\n",
      "iteration：3885 , batch_loss：0.3253727\n",
      "iteration：3890 , batch_loss：0.32523042\n",
      "iteration：3895 , batch_loss：0.32667524\n",
      "iteration：3900 , batch_loss：0.32640988\n",
      "iteration：3900 , acc：0.1275\n",
      "iteration：3905 , batch_loss：0.32594103\n",
      "iteration：3910 , batch_loss：0.3255249\n",
      "iteration：3915 , batch_loss：0.32541662\n",
      "iteration：3920 , batch_loss：0.3256147\n",
      "iteration：3925 , batch_loss：0.32591486\n",
      "iteration：3930 , batch_loss：0.3263201\n",
      "iteration：3935 , batch_loss：0.32579684\n",
      "iteration：3940 , batch_loss：0.32573137\n",
      "iteration：3945 , batch_loss：0.32517034\n",
      "iteration：3950 , batch_loss：0.32621375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration：3955 , batch_loss：0.3258315\n",
      "iteration：3960 , batch_loss：0.3269542\n",
      "iteration：3965 , batch_loss：0.32695517\n",
      "iteration：3970 , batch_loss：0.3257038\n",
      "iteration：3975 , batch_loss：0.32560173\n",
      "iteration：3980 , batch_loss：0.3260687\n",
      "iteration：3985 , batch_loss：0.3243574\n",
      "iteration：3990 , batch_loss：0.32556853\n",
      "iteration：3995 , batch_loss：0.32612124\n",
      "iteration：4000 , batch_loss：0.32649463\n",
      "iteration：4000 , acc：0.105\n",
      "iteration：4005 , batch_loss：0.32497263\n",
      "iteration：4010 , batch_loss：0.3252012\n",
      "iteration：4015 , batch_loss：0.32626295\n",
      "iteration：4020 , batch_loss：0.32643205\n",
      "iteration：4025 , batch_loss：0.32624623\n",
      "iteration：4030 , batch_loss：0.32588702\n",
      "iteration：4035 , batch_loss：0.32503116\n",
      "iteration：4040 , batch_loss：0.3251632\n",
      "iteration：4045 , batch_loss：0.32607085\n",
      "iteration：4050 , batch_loss：0.3258992\n",
      "iteration：4055 , batch_loss：0.3245302\n",
      "iteration：4060 , batch_loss：0.3254283\n",
      "iteration：4065 , batch_loss：0.32581204\n",
      "iteration：4070 , batch_loss：0.32518977\n",
      "iteration：4075 , batch_loss：0.32613835\n",
      "iteration：4080 , batch_loss：0.3272131\n",
      "iteration：4085 , batch_loss：0.3252501\n",
      "iteration：4090 , batch_loss：0.3263606\n",
      "iteration：4095 , batch_loss：0.32604355\n",
      "iteration：4100 , batch_loss：0.32643014\n",
      "iteration：4100 , acc：0.125\n",
      "iteration：4105 , batch_loss：0.3265547\n",
      "iteration：4110 , batch_loss：0.3254799\n",
      "iteration：4115 , batch_loss：0.32581365\n",
      "iteration：4120 , batch_loss：0.3257854\n",
      "iteration：4125 , batch_loss：0.32480177\n",
      "iteration：4130 , batch_loss：0.32573426\n",
      "iteration：4135 , batch_loss：0.3257134\n",
      "iteration：4140 , batch_loss：0.3255267\n",
      "iteration：4145 , batch_loss：0.32480338\n",
      "iteration：4150 , batch_loss：0.32543445\n",
      "iteration：4155 , batch_loss：0.3263023\n",
      "iteration：4160 , batch_loss：0.3258203\n",
      "iteration：4165 , batch_loss：0.32467136\n",
      "iteration：4170 , batch_loss：0.32497224\n",
      "iteration：4175 , batch_loss：0.3272935\n",
      "iteration：4180 , batch_loss：0.3255318\n",
      "iteration：4185 , batch_loss：0.32640222\n",
      "iteration：4190 , batch_loss：0.32554033\n",
      "iteration：4195 , batch_loss：0.32532486\n",
      "iteration：4200 , batch_loss：0.3248542\n",
      "iteration：4200 , acc：0.1175\n",
      "iteration：4205 , batch_loss：0.32602033\n",
      "iteration：4210 , batch_loss：0.32550403\n",
      "iteration：4215 , batch_loss：0.32538146\n",
      "iteration：4220 , batch_loss：0.32620618\n",
      "iteration：4225 , batch_loss：0.32591504\n",
      "iteration：4230 , batch_loss：0.32416072\n",
      "iteration：4235 , batch_loss：0.32545987\n",
      "iteration：4240 , batch_loss：0.32639217\n",
      "iteration：4245 , batch_loss：0.32533434\n",
      "iteration：4250 , batch_loss：0.3263051\n",
      "iteration：4255 , batch_loss：0.32545847\n",
      "iteration：4260 , batch_loss：0.3268419\n",
      "iteration：4265 , batch_loss：0.32551295\n",
      "iteration：4270 , batch_loss：0.32615805\n",
      "iteration：4275 , batch_loss：0.32561806\n",
      "iteration：4280 , batch_loss：0.32473758\n",
      "iteration：4285 , batch_loss：0.3261117\n",
      "iteration：4290 , batch_loss：0.32631832\n",
      "iteration：4295 , batch_loss：0.3252209\n",
      "iteration：4300 , batch_loss：0.3256957\n",
      "iteration：4300 , acc：0.08\n",
      "iteration：4305 , batch_loss：0.32545263\n",
      "iteration：4310 , batch_loss：0.32632902\n",
      "iteration：4315 , batch_loss：0.32678896\n",
      "iteration：4320 , batch_loss：0.3255264\n",
      "iteration：4325 , batch_loss：0.3269185\n",
      "iteration：4330 , batch_loss：0.32522762\n",
      "iteration：4335 , batch_loss：0.32600635\n",
      "iteration：4340 , batch_loss：0.32582444\n",
      "iteration：4345 , batch_loss：0.32595712\n",
      "iteration：4350 , batch_loss：0.3246789\n",
      "iteration：4355 , batch_loss：0.32633975\n",
      "iteration：4360 , batch_loss：0.32562545\n",
      "iteration：4365 , batch_loss：0.32585856\n",
      "iteration：4370 , batch_loss：0.32528377\n",
      "iteration：4375 , batch_loss：0.3260597\n",
      "iteration：4380 , batch_loss：0.3264287\n",
      "iteration：4385 , batch_loss：0.325599\n",
      "iteration：4390 , batch_loss：0.32552916\n",
      "iteration：4395 , batch_loss：0.32697597\n",
      "iteration：4400 , batch_loss：0.32525617\n",
      "iteration：4400 , acc：0.1125\n",
      "iteration：4405 , batch_loss：0.3254723\n",
      "iteration：4410 , batch_loss：0.32508793\n",
      "iteration：4415 , batch_loss：0.32438022\n",
      "iteration：4420 , batch_loss：0.32577103\n",
      "iteration：4425 , batch_loss：0.3249155\n",
      "iteration：4430 , batch_loss：0.32490572\n",
      "iteration：4435 , batch_loss：0.3239554\n",
      "iteration：4440 , batch_loss：0.32595125\n",
      "iteration：4445 , batch_loss：0.32518387\n",
      "iteration：4450 , batch_loss：0.3250437\n",
      "iteration：4455 , batch_loss：0.32518834\n",
      "iteration：4460 , batch_loss：0.325152\n",
      "iteration：4465 , batch_loss：0.32616302\n",
      "iteration：4470 , batch_loss：0.32529002\n",
      "iteration：4475 , batch_loss：0.32673776\n",
      "iteration：4480 , batch_loss：0.3252275\n",
      "iteration：4485 , batch_loss：0.3257227\n",
      "iteration：4490 , batch_loss：0.32515636\n",
      "iteration：4495 , batch_loss：0.32637134\n",
      "iteration：4500 , batch_loss：0.32621023\n",
      "iteration：4500 , acc：0.13\n",
      "iteration：4505 , batch_loss：0.32610554\n",
      "iteration：4510 , batch_loss：0.32468677\n",
      "iteration：4515 , batch_loss：0.3255144\n",
      "iteration：4520 , batch_loss：0.3256839\n",
      "iteration：4525 , batch_loss：0.3244306\n",
      "iteration：4530 , batch_loss：0.3254465\n",
      "iteration：4535 , batch_loss：0.32575816\n",
      "iteration：4540 , batch_loss：0.3245712\n",
      "iteration：4545 , batch_loss：0.32618475\n",
      "iteration：4550 , batch_loss：0.3252635\n",
      "iteration：4555 , batch_loss：0.32660022\n",
      "iteration：4560 , batch_loss：0.32569852\n",
      "iteration：4565 , batch_loss：0.3258422\n",
      "iteration：4570 , batch_loss：0.32482848\n",
      "iteration：4575 , batch_loss：0.32479993\n",
      "iteration：4580 , batch_loss：0.32663813\n",
      "iteration：4585 , batch_loss：0.32507724\n",
      "iteration：4590 , batch_loss：0.3263145\n",
      "iteration：4595 , batch_loss：0.32662696\n",
      "iteration：4600 , batch_loss：0.32436228\n",
      "iteration：4600 , acc：0.0825\n",
      "iteration：4605 , batch_loss：0.32610947\n",
      "iteration：4610 , batch_loss：0.3262009\n",
      "iteration：4615 , batch_loss：0.32586122\n",
      "iteration：4620 , batch_loss：0.32663944\n",
      "iteration：4625 , batch_loss：0.32705\n",
      "iteration：4630 , batch_loss：0.32598814\n",
      "iteration：4635 , batch_loss：0.32502037\n",
      "iteration：4640 , batch_loss：0.32589605\n",
      "iteration：4645 , batch_loss：0.32538643\n",
      "iteration：4650 , batch_loss：0.3261643\n",
      "iteration：4655 , batch_loss：0.3242303\n",
      "iteration：4660 , batch_loss：0.3263655\n",
      "iteration：4665 , batch_loss：0.3257696\n",
      "iteration：4670 , batch_loss：0.32573497\n",
      "iteration：4675 , batch_loss：0.3250607\n",
      "iteration：4680 , batch_loss：0.32540312\n",
      "iteration：4685 , batch_loss：0.32525262\n",
      "iteration：4690 , batch_loss：0.3257156\n",
      "iteration：4695 , batch_loss：0.32648093\n",
      "iteration：4700 , batch_loss：0.32474023\n",
      "iteration：4700 , acc：0.14\n",
      "iteration：4705 , batch_loss：0.32514834\n",
      "iteration：4710 , batch_loss：0.32501042\n",
      "iteration：4715 , batch_loss：0.32634538\n",
      "iteration：4720 , batch_loss：0.3263014\n",
      "iteration：4725 , batch_loss：0.32651216\n",
      "iteration：4730 , batch_loss：0.32513842\n",
      "iteration：4735 , batch_loss：0.32493705\n",
      "iteration：4740 , batch_loss：0.32510605\n",
      "iteration：4745 , batch_loss：0.32594913\n",
      "iteration：4750 , batch_loss：0.32544297\n",
      "iteration：4755 , batch_loss：0.3258204\n",
      "iteration：4760 , batch_loss：0.32547817\n",
      "iteration：4765 , batch_loss：0.32602558\n",
      "iteration：4770 , batch_loss：0.32555532\n",
      "iteration：4775 , batch_loss：0.32626957\n",
      "iteration：4780 , batch_loss：0.32638544\n",
      "iteration：4785 , batch_loss：0.3260206\n",
      "iteration：4790 , batch_loss：0.32522574\n",
      "iteration：4795 , batch_loss：0.32649642\n",
      "iteration：4800 , batch_loss：0.3257462\n",
      "iteration：4800 , acc：0.12\n",
      "iteration：4805 , batch_loss：0.32539862\n",
      "iteration：4810 , batch_loss：0.32554492\n",
      "iteration：4815 , batch_loss：0.32565156\n",
      "iteration：4820 , batch_loss：0.3248382\n",
      "iteration：4825 , batch_loss：0.3256143\n",
      "iteration：4830 , batch_loss：0.32614836\n",
      "iteration：4835 , batch_loss：0.32667798\n",
      "iteration：4840 , batch_loss：0.3258931\n",
      "iteration：4845 , batch_loss：0.32619166\n",
      "iteration：4850 , batch_loss：0.32406998\n",
      "iteration：4855 , batch_loss：0.325996\n",
      "iteration：4860 , batch_loss：0.32699862\n",
      "iteration：4865 , batch_loss：0.32553938\n",
      "iteration：4870 , batch_loss：0.32535282\n",
      "iteration：4875 , batch_loss：0.32550567\n",
      "iteration：4880 , batch_loss：0.32604766\n",
      "iteration：4885 , batch_loss：0.32546908\n",
      "iteration：4890 , batch_loss：0.32559747\n",
      "iteration：4895 , batch_loss：0.32574725\n",
      "iteration：4900 , batch_loss：0.32569137\n",
      "iteration：4900 , acc：0.0875\n",
      "iteration：4905 , batch_loss：0.32529244\n",
      "iteration：4910 , batch_loss：0.32672018\n",
      "iteration：4915 , batch_loss：0.32560712\n",
      "iteration：4920 , batch_loss：0.32590288\n",
      "iteration：4925 , batch_loss：0.32514837\n",
      "iteration：4930 , batch_loss：0.32519978\n",
      "iteration：4935 , batch_loss：0.32594317\n"
     ]
    }
   ],
   "source": [
    "# 将预测值规整成BATCH_SIZE个TEXT_LEN行len(CHAR_SET)列的矩阵，这样每行就对应字符序列中的一个字符的标签\n",
    "output = cnn(X, IMAGE_WIDTH, IMAGE_HEIGHT, TEXT_LEN, len(CHAR_SET))\n",
    "# 定义loss函数和优化算法\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=Y))\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "# 预测值\n",
    "predict = tf.argmax(tf.reshape(output, [-1, TEXT_LEN, len(CHAR_SET)]), 2)\n",
    "# 将真实标签规整成BATCH_SIZE个TEXT_LEN行len(CHAR_SET)列的矩阵，这样每行就对应字符序列中的一个字符的标签\n",
    "real_value = tf.argmax(tf.reshape(Y, [-1, TEXT_LEN, len(CHAR_SET)]), 2)\n",
    "# 计算预测准确率\n",
    "correct_pre = tf.equal(predict, real_value)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
    "\n",
    "# 保存模型的对象saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess_train:\n",
    "   sess_train.run(tf.global_variables_initializer())\n",
    "   if os.path.exists(\"./tmp/checkpoint\"):\n",
    "      # 判断模型是否存在，如果存在则从模型中恢复变量\n",
    "      saver.restore(sess_train, tf.train.latest_checkpoint(\"./tmp/\"))\n",
    "   step = 0\n",
    "   while True:\n",
    "      batch_x_train, batch_y_train = get_next_batch(BATCH_SIZE)\n",
    "      _, batch_loss = sess_train.run([train_op, loss],\n",
    "                                     feed_dict={X: batch_x_train, Y: batch_y_train, keep_prob: 0.75})\n",
    "      if step % 5 == 0:\n",
    "         # 每训练5次打印一次loss值\n",
    "         print(\"iteration：%d , batch_loss：%s\" % (step, batch_loss))\n",
    "      if step % 100 == 0:\n",
    "         # 每训练100次保存一次模型\n",
    "         saver.save(sess_train, \"./tmp/train_model\", global_step=step)\n",
    "         # 每训练100次计算并打印一次准确率\n",
    "         batch_x_validation, batch_y_validation = get_next_batch(VALIDATION_SIZE, mode=\"validation\")\n",
    "         acc = sess_train.run(accuracy, feed_dict={X: batch_x_validation, Y: batch_y_validation, keep_prob: 1})\n",
    "         print(\"iteration：%d , acc：%s\" % (step, acc))\n",
    "         # 如果准确率大于设定值,保存模型,完成训练\n",
    "         if acc > 0.9:\n",
    "            saver.save(sess_train, \"./tmp/train_model\", global_step=step)\n",
    "            break\n",
    "      step = step + 1\n",
    "# 建立用于测试模型的Session会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess_test:\n",
    "   sess_test.run(tf.global_variables_initializer())\n",
    "   saver.restore(sess_test, tf.train.latest_checkpoint(\"./tmp/\"))\n",
    "   for i in range(ITERATION):\n",
    "      batch_x_test, batch_y_test = get_next_batch(TEST_SIZE, mode=\"test\")\n",
    "      pred, real, acc = sess_test.run([predict, real_value, accuracy],\n",
    "                                      feed_dict={X: batch_x_test, Y: batch_y_test, keep_prob: 1})\n",
    "      # 注意pred和real都是tensorflow的张量tensor，虽然它们和多维数组形式非常类似，但不是数组，不是可迭代对象\n",
    "      # tensorflow的张量tensor不能直接用在for循环里迭代，另外数组中的元素如果是float型不能直接用作数组的下标\n",
    "      pred_int = tf.cast(pred, tf.int32)\n",
    "      real_int = tf.cast(real, tf.int32)\n",
    "      pred_array = pred_int.eval(session=sess_test)\n",
    "      real_array = real_int.eval(session=sess_test)\n",
    "      # pred, real的张量形式是2维矩阵，TEXT_SIZE行X4列矩阵，之所以是4列是因为前面用tf.argmax逐行提取了每行最大值的下标，一共四行，所以是4个下标\n",
    "      # 上面两步先将张量的值全部转为int型，然后将张量转换成多维数组\n",
    "      # 把张量转换成多维数组后，我们就可以进行迭代，然后使用pred_label_turn_to_char_list函数将标签转换成对应的字符序列\n",
    "      for j in range(TEST_SIZE):\n",
    "         # 这里预测值的标签经过tf.argmax只提取出了四个字符的下标，真实值的标签也这么处理了\n",
    "         # 因此由标签得到字符序列都使用函数pred_label_turn_to_char_list\n",
    "         pred_char_list = pred_label_turn_to_char_list(pred_array[j], TEXT_LEN, CHAR_SET)\n",
    "         real_char_list = pred_label_turn_to_char_list(real_array[j], TEXT_LEN, CHAR_SET)\n",
    "         print(\"第{}轮ITERATION中第{}个验证码预测：预测验证码为：{} 真实验证码为：{}\".format(i + 1, j + 1, pred_char_list, real_char_list))\n",
    "      print(\"第{}轮ITERATION识别测试正确率：{}\".format(i + 1, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
