{"cells":[{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# 输入数据 "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"import tensorflow as tf"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-3-7790fedc0ee5>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"}],"source":"#MNIST数据集下载\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Extracting MNISTdata/train-images-idx3-ubyte.gz\nExtracting MNISTdata/train-labels-idx1-ubyte.gz\nExtracting MNISTdata/t10k-images-idx3-ubyte.gz\nExtracting MNISTdata/t10k-labels-idx1-ubyte.gz\ntrain_images_shape: (55000, 784)\ntrain_labels_shape: (55000, 10)\ntest_images_shape: (10000, 784)\ntest_labels_shape: (10000, 10)\ntrain_images: [0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.3803922  0.37647063 0.3019608\n 0.46274513 0.2392157  0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.3529412\n 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n 0.9215687  0.74509805 0.08235294 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n 0.7411765  0.09019608 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n 0.08235294 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.14901961 0.32156864\n 0.0509804  0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.32941177\n 0.9960785  0.9960785  0.9176471  0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.32941177 0.9960785  0.9960785\n 0.9176471  0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.09803922\n 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n 0.9960785  0.5568628  0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.14509805 0.73333335 0.9921569\n 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n 0.34901962 0.12156864 0.         0.         0.         0.\n 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.6627451  0.9960785\n 0.6901961  0.24313727 0.         0.         0.         0.\n 0.         0.         0.         0.18823531 0.9058824  0.9960785\n 0.9176471  0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.07058824 0.48627454 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.54509807\n 0.9960785  0.9333334  0.22352943 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.34901962 0.9843138  0.9450981\n 0.3372549  0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.01568628 0.45882356\n 0.27058825 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ]\ntrain_images_length: 784\ntrain_labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"}],"source":"#one_hot 独热编码，也叫一位有效编码。在任意时候只有一位为1，其他位都是0\nmnist = input_data.read_data_sets(\"MNISTdata/\",one_hot=True)\ntrain_images = mnist.train.images\ntrain_labels = mnist.train.labels\ntest_images = mnist.test.images\ntest_labels = mnist.test.labels\nprint(\"train_images_shape:\", train_images.shape)\nprint(\"train_labels_shape:\", train_labels.shape)\nprint(\"test_images_shape:\", test_images.shape)\nprint(\"test_labels_shape:\", test_labels.shape)\nprint(\"train_images:\",train_images[0]) \nprint(\"train_images_length:\",len(train_images[0]))\nprint(\"train_labels:\", train_labels[0])"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# 预备知识"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"import tensorflow as tf\nimport numpy as np\n\n#占位符，不知道参数时使用\nx = tf.placeholder(tf.float32,shape=(4,4))\ny = tf.add(x,x)\n\n# [1,  32, 44, 56]\n# [89, 12, 90, 33]\n# [35, 69, 1,  10]\nargmax_paramter = tf.Variable([[1,32,44,56],[89,12,90,33],[35,69,1,10]])\n\n#最大列索引\nargmax_0 = tf.argmax(argmax_paramter,0)\n#最大行索引\nargmax_1 = tf.argmax(argmax_paramter,1)\n\n#平均数\nreduce_0 = tf.reduce_mean(argmax_paramter,reduction_indices=0)\nreduce_1 = tf.reduce_mean(argmax_paramter,reduction_indices=1)\n\n#相等\nequal_0 = tf.equal(1,2)\nequal_1 = tf.equal(2,2)\n\n#类型转换\ncast_0 = tf.cast(equal_0,tf.int32)\ncast_1 = tf.cast(equal_1,tf.int32)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[1.9523451  1.9298649  0.31272668 0.95004284]\n [1.5308484  0.84479403 1.5189203  1.3157444 ]\n [0.13467778 1.6916772  0.8058914  0.4949702 ]\n [1.2053528  1.2794371  1.0401701  1.547675  ]]\nargmax_0: [1 2 1 0]\nargmax_1: [3 2 1]\nreduce_0: [41 37 45 33]\nreduce_1: [33 56 28]\nequal_0: False\nequal_1: True\ncast_0: 0\ncast_1: 1\n"}],"source":"with tf.Session() as sess:\n    init = tf.global_variables_initializer();\n    sess.run(init)\n\n    rand_array = np.random.rand(4,4)\n    print(sess.run(y, feed_dict={x: rand_array}))\n    \n\n    print(\"argmax_0:\",sess.run(argmax_0))\n    print(\"argmax_1:\",sess.run(argmax_1))\n    print(\"reduce_0:\",sess.run(reduce_0))\n    print(\"reduce_1:\",sess.run(reduce_1))\n    print(\"equal_0:\",sess.run(equal_0))\n    print(\"equal_1:\",sess.run(equal_1))\n    print(\"cast_0:\",sess.run(cast_0))\n    print(\"cast_1:\",sess.run(cast_1))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# 逻辑回归方法"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## part1"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Extracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"}],"source":"import tensorflow as tf\n\n# 导入数据集\n#from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\n# 变量\nbatch_size = 100\n\n#训练的x(image),y(label)\n# x = tf.Variable()\n# y = tf.Variable()\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\n\n# 模型权重\n#[55000,784] * W = [55000,10]\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\n# 用softmax构建逻辑回归模型\npred = tf.nn.softmax(tf.matmul(x, W) + b)\n\n# 损失函数(交叉熵)\ncost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), 1))\n\n# 低度下降\noptimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n\n# 初始化所有变量\ninit = tf.global_variables_initializer()"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## part2"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Epoch: 0005 cost= 0.463733863\nEpoch: 0010 cost= 0.390899837\nEpoch: 0015 cost= 0.361415726\nEpoch: 0020 cost= 0.344135670\nEpoch: 0025 cost= 0.332493041\n运行完成\n正确率: 0.9141\n"}],"source":"# 加载session图\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # 开始训练\n    for epoch in range(25):\n        avg_cost = 0.\n        \n        total_batch = int(mnist.train.num_examples/batch_size)\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            sess.run(optimizer, {x: batch_xs,y: batch_ys})\n            #计算损失平均值\n            avg_cost += sess.run(cost,{x: batch_xs,y: batch_ys}) / total_batch\n        if (epoch+1) % 5 == 0:\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n\n    print(\"运行完成\")\n\n    # 测试求正确率\n    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n    print(\"正确率:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# 人工神经网络方法"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Extracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"}],"source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\ndef model(X, w_h, w_o):\n    h = tf.nn.sigmoid(tf.matmul(X, w_h)) # this is a basic mlp, think 2 stacked logistic regressions\n    return tf.matmul(h, w_o) # note that we dont take the softmax at the end because our cost fn does that for us\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-10-ef6f0607341b>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\n"}],"source":"X = tf.placeholder(\"float\", [None, 784])\nY = tf.placeholder(\"float\", [None, 10])\n\nw_h = init_weights([784, 625]) # create symbolic variables\nw_o = init_weights([625, 10])\n\npy_x = model(X, w_h, w_o)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y)) # compute costs\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # construct an optimizer\npredict_op = tf.argmax(py_x, 1)"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0 0.6845\n1 0.8217\n2 0.8606\n3 0.8806\n4 0.8883\n5 0.8939\n6 0.8982\n7 0.9009\n8 0.9038\n9 0.9058\n"}],"source":"# Launch the graph in a session\nwith tf.Session() as sess:\n    # you need to initialize all variables\n    tf.global_variables_initializer().run()\n\n    for i in range(10):\n        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n        print(i, np.mean(np.argmax(teY, axis=1) ==\n                         sess.run(predict_op, feed_dict={X: teX})))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# 卷积神经网络CNN方法"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## part1"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Extracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From <ipython-input-12-0d49fe84e0a2>:24: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n"}],"source":"# 加载TF 并加载数据\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n#设置输入参数\nbatch_size = 128\ntest_size = 256\n\n# 初始化权值与定义网络结构，建构一个3个卷积层和3个池化层\n#一个全连接层和一个输出层的卷积神经网络\n# 首先定义初始化权重函数\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\n\n# 第一组卷积层以及池化层，最后　droupout是为了防止过拟合，在模型训练的时候丢掉一些神经元\n# padding表示对边界的处理，SAME表示卷积的输入和输出保持同样尺寸\ndef model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden):\n    l1a = tf.nn.relu(tf.nn.conv2d(X, w,                       # l1a shape=(?, 28, 28, 32)\n                        strides=[1, 1, 1, 1], padding='SAME'))#水平滑动和竖直滑动的步长，填充输入输出图片大小不变\n    l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1],              # l1 shape=(?, 14, 14, 32)\n                        strides=[1, 2, 2, 1], padding='SAME')\n    l1 = tf.nn.dropout(l1, p_keep_conv)\n\n    # 第二组卷积层及池化层，最后dropout一些神经元\n    l2a = tf.nn.relu(tf.nn.conv2d(l1, w2,                     # l2a shape=(?, 14, 14, 64)\n                        strides=[1, 1, 1, 1], padding='SAME'))\n    l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1],              # l2 shape=(?, 7, 7, 64)\n                        strides=[1, 2, 2, 1], padding='SAME')\n    l2 = tf.nn.dropout(l2, p_keep_conv)\n    \n    # 第三组卷积神经网络及池化层，同样，最后dropout一些神经元\n    l3a = tf.nn.relu(tf.nn.conv2d(l2, w3,                     # l3a shape=(?, 7, 7, 128)\n                        strides=[1, 1, 1, 1], padding='SAME'))\n    l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1],              # l3 shape=(?, 4, 4, 128)\n                        strides=[1, 2, 2, 1], padding='SAME')\n    l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]])    # reshape to (?, 2048)\n    l3 = tf.nn.dropout(l3, p_keep_conv)\n\n    # 全连接层\n    l4 = tf.nn.relu(tf.matmul(l3, w4))\n    l4 = tf.nn.dropout(l4, p_keep_hidden)\n\n    # 输出层\n    pyx = tf.matmul(l4, w_o)\n    return pyx\n\n\n# 导入数据\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n# 定义四个变量，分别为输入训练图像矩阵及其标签，输入测试图像矩阵及其标签\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n# -1表示布考虑输入图片的数量，28*28为图片的像素数，1是通道(channel)的数量，\n# 因MNIST图片为黑白，彩色图片通道是3\ntrX = trX.reshape(-1, 28, 28, 1)  # 28x28x1 input img\nteX = teX.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n# 10为识别图片的类别从0到9，共10个取值\nX = tf.placeholder(\"float\", [None, 28, 28, 1])\nY = tf.placeholder(\"float\", [None, 10])\n\n# 定义模型函数\n# 神经网络模型的构建函数，传入以下参数\n# X：输入数据\n# w: 每一层权重\nw = init_weights([3, 3, 1, 32])       # 3x3x1 conv, 32 outputs\nw2 = init_weights([3, 3, 32, 64])     # 3x3x32 conv, 64 outputs\nw3 = init_weights([3, 3, 64, 128])    # 3x3x32 conv, 128 outputs\nw4 = init_weights([128 * 4 * 4, 625]) # FC 128 * 4 * 4 inputs, 625 outputs\nw_o = init_weights([625, 10])         # FC 625 inputs, 10 outputs (labels)\n\n# p_keep_conv,p_keep_hidden:dropout 保留神经元比例\n# 定义dropout的占位符keep_conv，表示一层中有多少比例的神经元被保留，生成网络模型，得到预测数据\n# 在训练的时候把设定比例的节点改为0，避免过拟合\np_keep_conv = tf.placeholder(\"float\")\np_keep_hidden = tf.placeholder(\"float\")\npy_x = model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden)\n# 定义损失函数，采用tf.nn.softmax_cross_entropy_with_logists，作为比较预测值和真实值的差距\n# 定义训练操作(train_op) 采用RMSProp算法作为优化器,\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\ntrain_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\npredict_op = tf.argmax(py_x, 1)"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## part2"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0 0.9296875\n1 0.984375\n2 0.9921875\n3 0.98828125\n4 0.98828125\n5 0.98828125\n6 0.9921875\n7 0.9921875\n8 0.99609375\n9 0.99609375\n"}],"source":"# Launch the graph in a session\nwith tf.Session() as sess:\n    # you need to initialize all variables\n    tf.global_variables_initializer().run()\n\n    for i in range(10):\n        training_batch = zip(range(0, len(trX), batch_size),\n                             range(batch_size, len(trX)+1, batch_size))\n        for start, end in training_batch:\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end],\n                                          p_keep_conv: 0.8, p_keep_hidden: 0.5})\n\n        test_indices = np.arange(len(teX)) # Get A Test Batch\n        np.random.shuffle(test_indices)\n        test_indices = test_indices[0:test_size]\n\n        print(i, np.mean(np.argmax(teY[test_indices], axis=1) ==\n                         sess.run(predict_op, feed_dict={X: teX[test_indices],\n                                                         Y: teY[test_indices],\n                                                         p_keep_conv: 1.0,\n                                                         p_keep_hidden: 1.0})))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# TensorFlow可视化"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Extracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"}],"source":"def init_weights(shape, name):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n\n# This network is the same as the previous one except with an extra hidden layer + dropout\ndef model(X, w_h, w_h2, w_o, p_keep_input, p_keep_hidden):\n    # Add layer name scopes for better graph visualization\n    with tf.name_scope(\"layer1\"):\n        X = tf.nn.dropout(X, p_keep_input)\n        h = tf.nn.relu(tf.matmul(X, w_h))\n    with tf.name_scope(\"layer2\"):\n        h = tf.nn.dropout(h, p_keep_hidden)\n        h2 = tf.nn.relu(tf.matmul(h, w_h2))\n    with tf.name_scope(\"layer3\"):\n        h2 = tf.nn.dropout(h2, p_keep_hidden)\n        return tf.matmul(h2, w_o)\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"X = tf.placeholder(\"float\", [None, 784], name=\"X\")\nY = tf.placeholder(\"float\", [None, 10], name=\"Y\")\n\nw_h = init_weights([784, 625], \"w_h\")\nw_h2 = init_weights([625, 625], \"w_h2\")\nw_o = init_weights([625, 10], \"w_o\")\n\n# Add histogram summaries for weights\ntf.summary.histogram(\"w_h_summ\", w_h)\ntf.summary.histogram(\"w_h2_summ\", w_h2)\ntf.summary.histogram(\"w_o_summ\", w_o)\n\np_keep_input = tf.placeholder(\"float\", name=\"p_keep_input\")\np_keep_hidden = tf.placeholder(\"float\", name=\"p_keep_hidden\")\npy_x = model(X, w_h, w_h2, w_o, p_keep_input, p_keep_hidden)\n\nwith tf.name_scope(\"cost\"):\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n    train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n    # Add scalar summary for cost\n    tf.summary.scalar(\"cost\", cost)\n\nwith tf.name_scope(\"accuracy\"):\n    correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(py_x, 1)) # Count correct predictions\n    acc_op = tf.reduce_mean(tf.cast(correct_pred, \"float\")) # Cast boolean to float to average\n    # Add scalar summary for accuracy\n    tf.summary.scalar(\"accuracy\", acc_op)"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0 0.9253\n1 0.9649\n2 0.9716\n3 0.9742\n4 0.9735\n5 0.9755\n6 0.977\n7 0.9773\n8 0.9787\n9 0.9815\n"}],"source":"with tf.Session() as sess:\n    # create a log writer. run 'tensorboard --logdir=./logs/nn_logs'\n    writer = tf.summary.FileWriter(\"./logs/nn_logs\", sess.graph)  \n    merged = tf.summary.merge_all()\n\n    # you need to initialize all variables\n    tf.global_variables_initializer().run()\n\n    for i in range(10):\n        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end],\n                                          p_keep_input: 0.8, p_keep_hidden: 0.5})\n        summary, acc = sess.run([merged, acc_op], feed_dict={X: teX, Y: teY,\n                                          p_keep_input: 1.0, p_keep_hidden: 1.0})\n        writer.add_summary(summary, i)  # Write summary\n        print(i, acc)                   # Report the accuracy"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# 保存神经网络"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\nC:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"}],"source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport os"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-2-bdd58e309248>:19: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"}],"source":"# This shows how to save/restore your model (trained variables).\n# To see how it works, please stop this program during training and resart.\n# This network is the same as 3_net.py\n\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\ndef model(X, w_h, w_h2, w_o, p_keep_input, p_keep_hidden): # this network is the same as the previous one except with an extra hidden layer + dropout\n    X = tf.nn.dropout(X, p_keep_input)\n    h = tf.nn.relu(tf.matmul(X, w_h))\n\n    h = tf.nn.dropout(h, p_keep_hidden)\n    h2 = tf.nn.relu(tf.matmul(h, w_h2))\n\n    h2 = tf.nn.dropout(h2, p_keep_hidden)\n\n    return tf.matmul(h2, w_o)\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-2-bdd58e309248>:9: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From <ipython-input-3-93871d90a2e1>:12: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n"}],"source":"X = tf.placeholder(\"float\", [None, 784])\nY = tf.placeholder(\"float\", [None, 10])\n\nw_h = init_weights([784, 625])\nw_h2 = init_weights([625, 625])\nw_o = init_weights([625, 10])\n\np_keep_input = tf.placeholder(\"float\")\np_keep_hidden = tf.placeholder(\"float\")\npy_x = model(X, w_h, w_h2, w_o, p_keep_input, p_keep_hidden)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\ntrain_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\npredict_op = tf.argmax(py_x, 1)\n\nckpt_dir = \"./ckpt_dir\"\nif not os.path.exists(ckpt_dir):\n    os.makedirs(ckpt_dir)\n\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\n\n# Call this after declaring all tf.Variables.\nsaver = tf.train.Saver()\n\n# This variable won't be stored, since it is declared after tf.train.Saver()\nnon_storable_variable = tf.Variable(777)"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Start from: 0\n0 0.9346\n1 0.9647\n2 0.9684\n3 0.9741\n4 0.9749\nWARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to delete files with this prefix.\n5 0.9787\n6 0.9775\n7 0.9792\n8 0.9777\n9 0.9778\n10 0.9815\n11 0.9808\n12 0.982\n13 0.9834\n14 0.9828\n15 0.9811\n16 0.9814\n17 0.9823\n18 0.9791\n19 0.9822\n20 0.9841\n21 0.9809\n22 0.983\n23 0.9836\n24 0.9804\n25 0.9829\n26 0.9825\n27 0.9816\n28 0.9832\n29 0.9851\n30 0.9836\n31 0.984\n32 0.9832\n33 0.9831\n34 0.9835\n35 0.9833\n36 0.9835\n37 0.9839\n38 0.9833\n39 0.9836\n40 0.983\n41 0.9832\n42 0.9827\n43 0.9833\n44 0.984\n45 0.9851\n46 0.9834\n47 0.9838\n48 0.9822\n49 0.9829\n50 0.9838\n51 0.9839\n52 0.9837\n53 0.9826\n54 0.9839\n55 0.9826\n56 0.9842\n57 0.9837\n58 0.9838\n59 0.9829\n60 0.9837\n61 0.9832\n62 0.9849\n63 0.9843\n64 0.9825\n65 0.9845\n66 0.984\n67 0.9841\n68 0.9839\n69 0.9836\n70 0.9848\n71 0.9841\n72 0.9856\n73 0.9857\n74 0.9846\n75 0.9843\n76 0.9825\n77 0.9827\n78 0.9833\n79 0.9847\n80 0.985\n81 0.9849\n82 0.9849\n83 0.9834\n84 0.9836\n85 0.9834\n86 0.9849\n87 0.9843\n88 0.9843\n89 0.9844\n90 0.9848\n91 0.9838\n92 0.9841\n93 0.9837\n94 0.9847\n95 0.9843\n96 0.9846\n97 0.9841\n98 0.985\n99 0.9854\n"}],"source":"# Launch the graph in a session\nwith tf.Session() as sess:\n    # you need to initialize all variables\n    tf.global_variables_initializer().run()\n\n    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n        print(ckpt.model_checkpoint_path)\n        saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n\n    start = global_step.eval() # get last global_step\n    print(\"Start from:\", start)\n\n    for i in range(start, 100):\n        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end],\n                                          p_keep_input: 0.8, p_keep_hidden: 0.5})\n\n        global_step.assign(i).eval() # set and update(eval) global_step with index, i\n        saver.save(sess, ckpt_dir + \"/model.ckpt\", global_step=global_step)\n        print(i, np.mean(np.argmax(teY, axis=1) ==\n                         sess.run(predict_op, feed_dict={X: teX, Y: teY,\n                                                         p_keep_input: 1.0,\n                                                         p_keep_hidden: 1.0})))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}