{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输入数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-7790fedc0ee5>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#MNIST数据集下载\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNISTdata/train-images-idx3-ubyte.gz\n",
      "Extracting MNISTdata/train-labels-idx1-ubyte.gz\n",
      "Extracting MNISTdata/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNISTdata/t10k-labels-idx1-ubyte.gz\n",
      "train_images_shape: (55000, 784)\n",
      "train_labels_shape: (55000, 10)\n",
      "test_images_shape: (10000, 784)\n",
      "test_labels_shape: (10000, 10)\n",
      "train_images: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "train_images_length: 784\n",
      "train_labels: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#one_hot 独热编码，也叫一位有效编码。在任意时候只有一位为1，其他位都是0\n",
    "mnist = input_data.read_data_sets(\"MNISTdata/\",one_hot=True)\n",
    "train_images = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "test_images = mnist.test.images\n",
    "test_labels = mnist.test.labels\n",
    "print(\"train_images_shape:\", train_images.shape)\n",
    "print(\"train_labels_shape:\", train_labels.shape)\n",
    "print(\"test_images_shape:\", test_images.shape)\n",
    "print(\"test_labels_shape:\", test_labels.shape)\n",
    "print(\"train_images:\",train_images[0]) \n",
    "print(\"train_images_length:\",len(train_images[0]))\n",
    "print(\"train_labels:\", train_labels[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预备知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#占位符，不知道参数时使用\n",
    "x = tf.placeholder(tf.float32,shape=(4,4))\n",
    "y = tf.add(x,x)\n",
    "\n",
    "# [1,  32, 44, 56]\n",
    "# [89, 12, 90, 33]\n",
    "# [35, 69, 1,  10]\n",
    "argmax_paramter = tf.Variable([[1,32,44,56],[89,12,90,33],[35,69,1,10]])\n",
    "\n",
    "#最大列索引\n",
    "argmax_0 = tf.argmax(argmax_paramter,0)\n",
    "#最大行索引\n",
    "argmax_1 = tf.argmax(argmax_paramter,1)\n",
    "\n",
    "#平均数\n",
    "reduce_0 = tf.reduce_mean(argmax_paramter,reduction_indices=0)\n",
    "reduce_1 = tf.reduce_mean(argmax_paramter,reduction_indices=1)\n",
    "\n",
    "#相等\n",
    "equal_0 = tf.equal(1,2)\n",
    "equal_1 = tf.equal(2,2)\n",
    "\n",
    "#类型转换\n",
    "cast_0 = tf.cast(equal_0,tf.int32)\n",
    "cast_1 = tf.cast(equal_1,tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3013859  0.60186255 0.28586176 0.22096151]\n",
      " [0.01295787 1.2120873  0.6522825  1.3644898 ]\n",
      " [0.9067774  0.10974703 0.03340472 0.6973365 ]\n",
      " [0.20343974 0.44320726 1.3298311  0.64129925]]\n",
      "argmax_0: [1 2 1 0]\n",
      "argmax_1: [3 2 1]\n",
      "reduce_0: [41 37 45 33]\n",
      "reduce_1: [33 56 28]\n",
      "equal_0: False\n",
      "equal_1: True\n",
      "cast_0: 0\n",
      "cast_1: 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer();\n",
    "    sess.run(init)\n",
    "\n",
    "    rand_array = np.random.rand(4,4)\n",
    "    print(sess.run(y, feed_dict={x: rand_array}))\n",
    "    \n",
    "\n",
    "    print(\"argmax_0:\",sess.run(argmax_0))\n",
    "    print(\"argmax_1:\",sess.run(argmax_1))\n",
    "    print(\"reduce_0:\",sess.run(reduce_0))\n",
    "    print(\"reduce_1:\",sess.run(reduce_1))\n",
    "    print(\"equal_0:\",sess.run(equal_0))\n",
    "    print(\"equal_1:\",sess.run(equal_1))\n",
    "    print(\"cast_0:\",sess.run(cast_0))\n",
    "    print(\"cast_1:\",sess.run(cast_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 导入数据集\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# 变量\n",
    "batch_size = 100\n",
    "\n",
    "#训练的x(image),y(label)\n",
    "# x = tf.Variable()\n",
    "# y = tf.Variable()\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 模型权重\n",
    "#[55000,784] * W = [55000,10]\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 用softmax构建逻辑回归模型\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# 损失函数(交叉熵)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), 1))\n",
    "\n",
    "# 低度下降\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "# 初始化所有变量\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0005 cost= 0.463764931\n",
      "Epoch: 0010 cost= 0.390926333\n",
      "Epoch: 0015 cost= 0.361357333\n",
      "Epoch: 0020 cost= 0.344095792\n",
      "Epoch: 0025 cost= 0.332450936\n",
      "运行完成\n",
      "正确率: 0.9137\n"
     ]
    }
   ],
   "source": [
    "# 加载session图\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(25):\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, {x: batch_xs,y: batch_ys})\n",
    "            #计算损失平均值\n",
    "            avg_cost += sess.run(cost,{x: batch_xs,y: batch_ys}) / total_batch\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"运行完成\")\n",
    "\n",
    "    # 测试求正确率\n",
    "    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    print(\"正确率:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 人工神经网络方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def model(X, w_h, w_o):\n",
    "    h = tf.nn.sigmoid(tf.matmul(X, w_h)) # this is a basic mlp, think 2 stacked logistic regressions\n",
    "    return tf.matmul(h, w_o) # note that we dont take the softmax at the end because our cost fn does that for us\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-ef6f0607341b>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, 784])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "w_h = init_weights([784, 625]) # create symbolic variables\n",
    "w_o = init_weights([625, 10])\n",
    "\n",
    "py_x = model(X, w_h, w_o)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y)) # compute costs\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # construct an optimizer\n",
    "predict_op = tf.argmax(py_x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6937\n",
      "1 0.8228\n",
      "2 0.8622\n",
      "3 0.8806\n",
      "4 0.8882\n",
      "5 0.8942\n",
      "6 0.8983\n",
      "7 0.9014\n",
      "8 0.9045\n",
      "9 0.9059\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(10):\n",
    "        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n",
    "            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n",
    "        print(i, np.mean(np.argmax(teY, axis=1) ==\n",
    "                         sess.run(predict_op, feed_dict={X: teX})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络CNN方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-12-c4b37995e263>:16: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\WYG\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 加载TF 并加载数据\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#设置输入参数\n",
    "batch_size = 128\n",
    "test_size = 256\n",
    "\n",
    "# 初始化权值与定义网络结构，建构一个3个卷积层和3个池化层\n",
    "#一个全连接层和一个输出层的卷积神经网络\n",
    "# 首先定义初始化权重函数\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "\n",
    "# 第一组卷积层以及池化层，最后　droupout是为了防止过拟合，在模型训练的时候丢掉一些神经元\n",
    "# padding表示对边界的处理，SAME表示卷积的输入和输出保持同样尺寸\n",
    "def model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden):\n",
    "    l1a = tf.nn.relu(tf.nn.conv2d(X, w,                       # l1a shape=(?, 28, 28, 32)\n",
    "                        strides=[1, 1, 1, 1], padding='SAME'))#水平滑动和竖直滑动的步长，填充输入输出图片大小不变\n",
    "    l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1],              # l1 shape=(?, 14, 14, 32)\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l1 = tf.nn.dropout(l1, p_keep_conv)\n",
    "\n",
    "    # 第二组卷积层及池化层，最后dropout一些神经元\n",
    "    l2a = tf.nn.relu(tf.nn.conv2d(l1, w2,                     # l2a shape=(?, 14, 14, 64)\n",
    "                        strides=[1, 1, 1, 1], padding='SAME'))\n",
    "    l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1],              # l2 shape=(?, 7, 7, 64)\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l2 = tf.nn.dropout(l2, p_keep_conv)\n",
    "    \n",
    "    # 第三组卷积神经网络及池化层，同样，最后dropout一些神经元\n",
    "    l3a = tf.nn.relu(tf.nn.conv2d(l2, w3,                     # l3a shape=(?, 7, 7, 128)\n",
    "                        strides=[1, 1, 1, 1], padding='SAME'))\n",
    "    l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1],              # l3 shape=(?, 4, 4, 128)\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]])    # reshape to (?, 2048)\n",
    "    l3 = tf.nn.dropout(l3, p_keep_conv)\n",
    "\n",
    "    # 全连接层\n",
    "    l4 = tf.nn.relu(tf.matmul(l3, w4))\n",
    "    l4 = tf.nn.dropout(l4, p_keep_hidden)\n",
    "\n",
    "    # 输出层\n",
    "    pyx = tf.matmul(l4, w_o)\n",
    "    return pyx\n",
    "\n",
    "\n",
    "# 导入数据\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# 定义四个变量，分别为输入训练图像矩阵及其标签，输入测试图像矩阵及其标签\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "# -1表示布考虑输入图片的数量，28*28为图片的像素数，1是通道(channel)的数量，\n",
    "# 因MNIST图片为黑白，彩色图片通道是3\n",
    "trX = trX.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n",
    "teX = teX.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n",
    "# 10为识别图片的类别从0到9，共10个取值\n",
    "X = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# 定义模型函数\n",
    "# 神经网络模型的构建函数，传入以下参数\n",
    "# X：输入数据\n",
    "# w: 每一层权重\n",
    "w = init_weights([3, 3, 1, 32])       # 3x3x1 conv, 32 outputs\n",
    "w2 = init_weights([3, 3, 32, 64])     # 3x3x32 conv, 64 outputs\n",
    "w3 = init_weights([3, 3, 64, 128])    # 3x3x32 conv, 128 outputs\n",
    "w4 = init_weights([128 * 4 * 4, 625]) # FC 128 * 4 * 4 inputs, 625 outputs\n",
    "w_o = init_weights([625, 10])         # FC 625 inputs, 10 outputs (labels)\n",
    "\n",
    "# p_keep_conv,p_keep_hidden:dropout 保留神经元比例\n",
    "# 定义dropout的占位符keep_conv，表示一层中有多少比例的神经元被保留，生成网络模型，得到预测数据\n",
    "# 在训练的时候把设定比例的节点改为0，避免过拟合\n",
    "p_keep_conv = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "py_x = model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden)\n",
    "# 定义损失函数，采用tf.nn.softmax_cross_entropy_with_logists，作为比较预测值和真实值的差距\n",
    "# 定义训练操作(train_op) 采用RMSProp算法作为优化器,\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n",
    "train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "predict_op = tf.argmax(py_x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.95703125\n",
      "1 0.9765625\n",
      "2 0.984375\n",
      "3 0.984375\n",
      "4 1.0\n",
      "5 0.99609375\n",
      "6 0.9921875\n",
      "7 0.9921875\n",
      "8 0.984375\n",
      "9 0.98828125\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(10):\n",
    "        training_batch = zip(range(0, len(trX), batch_size),\n",
    "                             range(batch_size, len(trX)+1, batch_size))\n",
    "        for start, end in training_batch:\n",
    "            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end],\n",
    "                                          p_keep_conv: 0.8, p_keep_hidden: 0.5})\n",
    "\n",
    "        test_indices = np.arange(len(teX)) # Get A Test Batch\n",
    "        np.random.shuffle(test_indices)\n",
    "        test_indices = test_indices[0:test_size]\n",
    "\n",
    "        print(i, np.mean(np.argmax(teY[test_indices], axis=1) ==\n",
    "                         sess.run(predict_op, feed_dict={X: teX[test_indices],\n",
    "                                                         Y: teY[test_indices],\n",
    "                                                         p_keep_conv: 1.0,\n",
    "                                                         p_keep_hidden: 1.0})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "def init_weights(shape, name):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01), name=name)\n",
    "\n",
    "# This network is the same as the previous one except with an extra hidden layer + dropout\n",
    "def model(X, w_h, w_h2, w_o, p_keep_input, p_keep_hidden):\n",
    "    # Add layer name scopes for better graph visualization\n",
    "    with tf.name_scope(\"layer1\"):\n",
    "        X = tf.nn.dropout(X, p_keep_input)\n",
    "        h = tf.nn.relu(tf.matmul(X, w_h))\n",
    "    with tf.name_scope(\"layer2\"):\n",
    "        h = tf.nn.dropout(h, p_keep_hidden)\n",
    "        h2 = tf.nn.relu(tf.matmul(h, w_h2))\n",
    "    with tf.name_scope(\"layer3\"):\n",
    "        h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "        return tf.matmul(h2, w_o)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 784], name=\"X\")\n",
    "Y = tf.placeholder(\"float\", [None, 10], name=\"Y\")\n",
    "\n",
    "w_h = init_weights([784, 625], \"w_h\")\n",
    "w_h2 = init_weights([625, 625], \"w_h2\")\n",
    "w_o = init_weights([625, 10], \"w_o\")\n",
    "\n",
    "# Add histogram summaries for weights\n",
    "tf.summary.histogram(\"w_h_summ\", w_h)\n",
    "tf.summary.histogram(\"w_h2_summ\", w_h2)\n",
    "tf.summary.histogram(\"w_o_summ\", w_o)\n",
    "\n",
    "p_keep_input = tf.placeholder(\"float\", name=\"p_keep_input\")\n",
    "p_keep_hidden = tf.placeholder(\"float\", name=\"p_keep_hidden\")\n",
    "py_x = model(X, w_h, w_h2, w_o, p_keep_input, p_keep_hidden)\n",
    "\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n",
    "    train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "    # Add scalar summary for cost\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(py_x, 1)) # Count correct predictions\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_pred, \"float\")) # Cast boolean to float to average\n",
    "    # Add scalar summary for accuracy\n",
    "    tf.summary.scalar(\"accuracy\", acc_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9378\n",
      "1 0.9643\n",
      "2 0.9708\n",
      "3 0.9737\n",
      "4 0.9775\n",
      "5 0.9769\n",
      "6 0.9788\n",
      "7 0.9769\n",
      "8 0.9788\n",
      "9 0.9783\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # create a log writer. run 'tensorboard --logdir=./logs/nn_logs'\n",
    "    writer = tf.summary.FileWriter(\"./logs/nn_logs\", sess.graph)  \n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(10):\n",
    "        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n",
    "            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end],\n",
    "                                          p_keep_input: 0.8, p_keep_hidden: 0.5})\n",
    "        summary, acc = sess.run([merged, acc_op], feed_dict={X: teX, Y: teY,\n",
    "                                          p_keep_input: 1.0, p_keep_hidden: 1.0})\n",
    "        writer.add_summary(summary, i)  # Write summary\n",
    "        print(i, acc)                   # Report the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# This shows how to save/restore your model (trained variables).\n",
    "# To see how it works, please stop this program during training and resart.\n",
    "# This network is the same as 3_net.py\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def model(X, w_h, w_h2, w_o, p_keep_input, p_keep_hidden): # this network is the same as the previous one except with an extra hidden layer + dropout\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    h = tf.nn.relu(tf.matmul(X, w_h))\n",
    "\n",
    "    h = tf.nn.dropout(h, p_keep_hidden)\n",
    "    h2 = tf.nn.relu(tf.matmul(h, w_h2))\n",
    "\n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "\n",
    "    return tf.matmul(h2, w_o)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 784])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "w_h = init_weights([784, 625])\n",
    "w_h2 = init_weights([625, 625])\n",
    "w_o = init_weights([625, 10])\n",
    "\n",
    "p_keep_input = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "py_x = model(X, w_h, w_h2, w_o, p_keep_input, p_keep_hidden)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\n",
    "train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "predict_op = tf.argmax(py_x, 1)\n",
    "\n",
    "ckpt_dir = \"./ckpt_dir\"\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Call this after declaring all tf.Variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# This variable won't be stored, since it is declared after tf.train.Saver()\n",
    "non_storable_variable = tf.Variable(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fdef7ba5186b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Launch the graph in a session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# you need to initialize all variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "\n",
    "    start = global_step.eval() # get last global_step\n",
    "    print(\"Start from:\", start)\n",
    "\n",
    "    for i in range(start, 100):\n",
    "        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n",
    "            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end],\n",
    "                                          p_keep_input: 0.8, p_keep_hidden: 0.5})\n",
    "\n",
    "        global_step.assign(i).eval() # set and update(eval) global_step with index, i\n",
    "        saver.save(sess, ckpt_dir + \"/model.ckpt\", global_step=global_step)\n",
    "        print(i, np.mean(np.argmax(teY, axis=1) ==\n",
    "                         sess.run(predict_op, feed_dict={X: teX, Y: teY,\n",
    "                                                         p_keep_input: 1.0,\n",
    "                                                         p_keep_hidden: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
